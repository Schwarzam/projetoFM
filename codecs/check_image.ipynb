{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e80521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# ðŸ”¹ MUST be before importing torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "# One of your parquet files\n",
    "PARQUET_FILE = \"/home/astrodados4/downloads/hypercube/datacube_HYDRA-0011.parquet\"\n",
    "\n",
    "# Trained model weights from your training script\n",
    "MODEL_PATH = \"conv_autoencoder_splus_qinco_spatial.pt\"\n",
    "\n",
    "# How many objects to visualize\n",
    "N_SAMPLES = 4\n",
    "\n",
    "# Which band to show\n",
    "BANDS = [\n",
    "    \"F378\", \"F395\", \"F410\", \"F430\",\n",
    "    \"F515\", \"F660\", \"F861\",\n",
    "    \"U\", \"G\", \"R\", \"I\", \"Z\",\n",
    "]\n",
    "PLOT_BAND = \"R\"  # must be in BANDS\n",
    "IMG_SIZE = 96\n",
    "\n",
    "# QINCo & latent config (must match training)\n",
    "LATENT_DIM = 8\n",
    "QINCO_USE = True\n",
    "QINCO_STEPS = 4\n",
    "QINCO_K = 1024\n",
    "\n",
    "# Require F378 not null, as in training\n",
    "REQUIRE_F378_NOT_NULL = True\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "\n",
    "_GRID_CACHE = {}\n",
    "\n",
    "def get_grid(H: int, W: int, device: torch.device):\n",
    "    key = (H, W, device.type)\n",
    "    if key not in _GRID_CACHE:\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(H, device=device),\n",
    "            torch.arange(W, device=device),\n",
    "            indexing=\"ij\"\n",
    "        )\n",
    "        _GRID_CACHE[key] = (yy.float(), xx.float())\n",
    "    return _GRID_CACHE[key]\n",
    "\n",
    "\n",
    "def _to_image_torch(flat) -> torch.Tensor:\n",
    "    \"\"\"Convert flattened array-like into a square image (H, W).\"\"\"\n",
    "    arr = torch.tensor(flat, dtype=torch.float32)\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        return arr\n",
    "\n",
    "    if arr.ndim == 1:\n",
    "        n = arr.numel()\n",
    "        side = int(math.isqrt(n))\n",
    "        if side * side != n:\n",
    "            raise ValueError(f\"Cannot reshape length {n} into a square image\")\n",
    "        return arr.view(side, side)\n",
    "\n",
    "    raise ValueError(f\"Unexpected ndim={arr.ndim} for image data\")\n",
    "\n",
    "\n",
    "def elliptical_mask(H, W, x0, y0, a, b, theta, device=\"cpu\", expand_factor=4.0):\n",
    "    \"\"\"Binary mask: 1 inside expanded ellipse, 0 outside.\"\"\"\n",
    "    device = torch.device(device)\n",
    "    yy, xx = get_grid(H, W, device=device)\n",
    "\n",
    "    a_scaled = a * expand_factor\n",
    "    b_scaled = b * expand_factor\n",
    "\n",
    "    X = xx - x0\n",
    "    Y = yy - y0\n",
    "\n",
    "    ct = torch.cos(theta)\n",
    "    st = torch.sin(theta)\n",
    "\n",
    "    Xp =  X * ct + Y * st\n",
    "    Yp = -X * st + Y * ct\n",
    "\n",
    "    mask = (Xp / a_scaled) ** 2 + (Yp / b_scaled) ** 2 <= 1.0\n",
    "    return mask.float()\n",
    "\n",
    "\n",
    "def percentile_range(values: np.ndarray, p_lo=1.0, p_hi=99.0):\n",
    "    \"\"\"Safe percentile-based range for plotting.\"\"\"\n",
    "    flat = values.reshape(-1)\n",
    "    flat = flat[np.isfinite(flat)]\n",
    "    if flat.size == 0:\n",
    "        return float(0.0), float(1.0)\n",
    "    v_lo = float(np.percentile(flat, p_lo))\n",
    "    v_hi = float(np.percentile(flat, p_hi))\n",
    "    if v_lo == v_hi:\n",
    "        v_lo = float(flat.min())\n",
    "        v_hi = float(flat.max())\n",
    "        if v_lo == v_hi:\n",
    "            v_hi = v_lo + 1.0\n",
    "    return v_lo, v_hi\n",
    "\n",
    "# =========================\n",
    "# QINCo MODULES (SAME AS TRAINING)\n",
    "# =========================\n",
    "\n",
    "class QINCoStep(nn.Module):\n",
    "    def __init__(self, D: int, K: int, hidden_dim: int = 256, num_res_blocks: int = 2):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "\n",
    "        self.base_codebook = nn.Parameter(torch.randn(K, D) * 0.1)\n",
    "        self.concat_proj = nn.Linear(2 * D, D)\n",
    "\n",
    "        blocks = []\n",
    "        for _ in range(num_res_blocks):\n",
    "            blocks.append(nn.Sequential(\n",
    "                nn.Linear(D, hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_dim, D),\n",
    "            ))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward_codebook(self, x_hat: torch.Tensor) -> torch.Tensor:\n",
    "        N, D = x_hat.shape\n",
    "        x_exp = x_hat.unsqueeze(1).expand(-1, self.K, -1)\n",
    "        cbar = self.base_codebook.unsqueeze(0).expand(N, -1, -1)\n",
    "        concat = torch.cat([x_exp, cbar], dim=-1)   # (N,K,2D)\n",
    "        C = self.concat_proj(concat)\n",
    "        for block in self.blocks:\n",
    "            C = C + block(C)\n",
    "        return C\n",
    "\n",
    "    def encode_step(self, x: torch.Tensor, x_hat: torch.Tensor):\n",
    "        N, D = x.shape\n",
    "        C = self.forward_codebook(x_hat)  # (N,K,D)\n",
    "\n",
    "        r = x - x_hat  # (N,D)\n",
    "        r_exp = r.unsqueeze(1).expand(-1, self.K, -1)\n",
    "        dists = torch.sum((r_exp - C) ** 2, dim=-1)  # (N,K)\n",
    "\n",
    "        codes = torch.argmin(dists, dim=-1)          # (N,)\n",
    "\n",
    "        C_flat = C.reshape(N * self.K, D)\n",
    "        idx = codes + torch.arange(N, device=x.device) * self.K\n",
    "        c_sel = C_flat[idx]                          # (N,D)\n",
    "\n",
    "        x_hat_new = x_hat + c_sel\n",
    "        return codes, x_hat_new, r, c_sel\n",
    "\n",
    "\n",
    "class QINCoQuantizer(nn.Module):\n",
    "    def __init__(self, D: int, K: int = 256, M: int = 4):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.steps = nn.ModuleList(\n",
    "            [QINCoStep(D, K) for _ in range(M)]\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        N, D = z.shape\n",
    "        x_hat = torch.zeros_like(z)\n",
    "\n",
    "        codes_all = []\n",
    "        residuals = []\n",
    "        selected_centroids = []\n",
    "\n",
    "        for step in self.steps:\n",
    "            codes, x_hat, r, c_sel = step.encode_step(z, x_hat)\n",
    "            codes_all.append(codes)\n",
    "            residuals.append(r)\n",
    "            selected_centroids.append(c_sel)\n",
    "\n",
    "        codes_all = torch.stack(codes_all, dim=-1)  # (N,M)\n",
    "        z_q = x_hat\n",
    "        z_q_st = z + (z_q - z).detach()\n",
    "\n",
    "        aux = {\n",
    "            \"residuals\": residuals,\n",
    "            \"centroids\": selected_centroids\n",
    "        }\n",
    "        return z_q_st, codes_all, aux\n",
    "\n",
    "\n",
    "class QINCoQuantizerSpatial(nn.Module):\n",
    "    def __init__(self, D: int, H: int, W: int, K: int = 256, M: int = 4):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.inner = QINCoQuantizer(D=D, K=K, M=M)\n",
    "\n",
    "    def forward(self, z_map: torch.Tensor):\n",
    "        B, D, H, W = z_map.shape\n",
    "        assert D == self.D and H == self.H and W == self.W\n",
    "\n",
    "        z_flat = z_map.permute(0, 2, 3, 1).reshape(-1, D)   # (N,D)\n",
    "        z_q_flat, codes_flat, aux = self.inner(z_flat)\n",
    "\n",
    "        z_q_map = z_q_flat.view(B, H, W, D).permute(0, 3, 1, 2)  # (B,D,H,W)\n",
    "        codes = codes_flat.view(B, H, W, -1)                     # (B,H,W,M)\n",
    "\n",
    "        return z_q_map, codes, aux\n",
    "\n",
    "# =========================\n",
    "# MODEL (SAME ARCH AS TRAINING)\n",
    "# =========================\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels: int = len(BANDS), latent_dim: int = LATENT_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 96 -> 48\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 48 -> 24\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 24 -> 12\n",
    "        )\n",
    "\n",
    "        self.latent_H = IMG_SIZE // 8\n",
    "        self.latent_W = IMG_SIZE // 8\n",
    "\n",
    "        self.to_latent = nn.Conv2d(128, latent_dim, kernel_size=1)\n",
    "        self.from_latent = nn.Conv2d(latent_dim, 128, kernel_size=1)\n",
    "\n",
    "        if QINCO_USE:\n",
    "            self.qinco = QINCoQuantizerSpatial(\n",
    "                D=latent_dim,\n",
    "                H=self.latent_H,\n",
    "                W=self.latent_W,\n",
    "                K=QINCO_K,\n",
    "                M=QINCO_STEPS,\n",
    "            )\n",
    "        else:\n",
    "            self.qinco = None\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),  # 12 -> 24\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),   # 24 -> 48\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(32, in_channels, kernel_size=2, stride=2),  # 48 -> 96\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder_conv(x)\n",
    "        z_map = self.to_latent(h)\n",
    "        return z_map\n",
    "\n",
    "    def decode(self, z_map):\n",
    "        h = self.from_latent(z_map)\n",
    "        x_hat = self.decoder(h)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_map = self.encode(x)\n",
    "        if self.qinco is not None:\n",
    "            z_q_map, codes, q_aux = self.qinco(z_map)\n",
    "            x_hat = self.decode(z_q_map)\n",
    "        else:\n",
    "            codes, q_aux = None, None\n",
    "            x_hat = self.decode(z_map)\n",
    "        return x_hat, z_map, codes, q_aux\n",
    "\n",
    "# =========================\n",
    "# MAIN: LOAD DF, MODEL, ENCODE/DECODE, PLOT\n",
    "# =========================\n",
    "\n",
    "def build_batch_from_df(df: pl.DataFrame, n_samples: int):\n",
    "    \"\"\"\n",
    "    Mimic the dataset logic for the first n_samples rows:\n",
    "    returns x_batch (N,C,H,W) and m_pix_batch (N,C,H,W).\n",
    "    \"\"\"\n",
    "    n = min(n_samples, df.height)\n",
    "    x_list = []\n",
    "    m_pix_list = []\n",
    "\n",
    "    for idx in range(4000, n + 4000):\n",
    "        imgs = []\n",
    "        masks_pix_binary = []\n",
    "        for band in BANDS:\n",
    "            flat = df[f\"splus_cut_{band}\"][idx]\n",
    "            img = _to_image_torch(flat)\n",
    "            valid = torch.isfinite(img) & (img != 0.0)\n",
    "            img_clean = img.clone()\n",
    "            img_clean[~torch.isfinite(img_clean)] = 0.0\n",
    "            imgs.append(img_clean)\n",
    "            masks_pix_binary.append(valid.float())\n",
    "\n",
    "        x = torch.stack(imgs, dim=0)  # (C,H,W)\n",
    "        m_pix_basic = torch.stack(masks_pix_binary, dim=0)\n",
    "\n",
    "        C, H, W = x.shape\n",
    "        device_cpu = torch.device(\"cpu\")\n",
    "        x = x.to(device_cpu)\n",
    "        m_pix_basic = m_pix_basic.to(device_cpu)\n",
    "\n",
    "        x0 = torch.tensor(IMG_SIZE // 2, dtype=torch.float32, device=device_cpu)\n",
    "        y0 = torch.tensor(IMG_SIZE // 2, dtype=torch.float32, device=device_cpu)\n",
    "        a  = torch.tensor(float(df[\"a_pixel_det\"][idx]), device=device_cpu)\n",
    "        b  = torch.tensor(float(df[\"b_pixel_det\"][idx]), device=device_cpu)\n",
    "        th = torch.tensor(float(df[\"theta_det\"][idx]), device=device_cpu)\n",
    "        theta = th * math.pi / 180.0\n",
    "\n",
    "        obj_mask = elliptical_mask(H, W, x0, y0, a, b, theta, device=device_cpu)\n",
    "        obj_mask_full = obj_mask.unsqueeze(0).expand(C, H, W)\n",
    "        m_pix = m_pix_basic * obj_mask_full\n",
    "\n",
    "        x_list.append(x)\n",
    "        m_pix_list.append(m_pix)\n",
    "\n",
    "    x_batch = torch.stack(x_list, dim=0)          # (N,C,H,W)\n",
    "    m_pix_batch = torch.stack(m_pix_list, dim=0)  # (N,C,H,W)\n",
    "    return x_batch, m_pix_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64db7df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----- 1) Load dataframe -----\n",
    "df = pl.read_parquet(PARQUET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117c07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in DF after filtering: 19331\n",
      "Model loaded from: conv_autoencoder_splus_qinco_spatial.pt\n",
      "x_batch shape: torch.Size([4, 12, 96, 96])\n",
      "x_hat_batch shape: torch.Size([4, 12, 96, 96])\n",
      "codes shape (B,H_lat,W_lat,M): torch.Size([4, 12, 12, 4])\n",
      "Saved plot to: inspect_plots/reconstruction_examples.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if REQUIRE_F378_NOT_NULL:\n",
    "    df = df.filter(pl.col(\"splus_cut_F378\").is_not_null())\n",
    "print(\"Rows in DF after filtering:\", df.height)\n",
    "if df.height == 0:\n",
    "    print(\"No rows to visualize, exiting.\")\n",
    "\n",
    "# ----- 2) Build model and load weights -----\n",
    "model = ConvAutoEncoder(in_channels=len(BANDS), latent_dim=LATENT_DIM).to(device)\n",
    "state = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "print(\"Model loaded from:\", MODEL_PATH)\n",
    "\n",
    "# ----- 3) Build a small batch from DF -----\n",
    "x_batch, m_pix_batch = build_batch_from_df(df, N_SAMPLES)\n",
    "x_batch = x_batch.to(device)\n",
    "m_pix_batch = m_pix_batch.to(device)\n",
    "\n",
    "# ----- 4) Encode / decode with QINCo -----\n",
    "with torch.no_grad():\n",
    "    x_hat_batch, z_map, codes, q_aux = model(x_batch)\n",
    "\n",
    "print(\"x_batch shape:\", x_batch.shape)      # (N,C,H,W)\n",
    "print(\"x_hat_batch shape:\", x_hat_batch.shape)\n",
    "if codes is not None:\n",
    "    print(\"codes shape (B,H_lat,W_lat,M):\", codes.shape)\n",
    "\n",
    "x_batch_cpu = x_batch.detach().cpu()\n",
    "x_hat_cpu = x_hat_batch.detach().cpu()\n",
    "m_pix_cpu = m_pix_batch.detach().cpu()\n",
    "\n",
    "# ----- 5) Plot original vs reconstruction vs residual -----\n",
    "band_idx = BANDS.index(PLOT_BAND)\n",
    "N = x_batch_cpu.shape[0]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    N, 3, figsize=(12, 4 * N),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for row in range(N):\n",
    "    x_orig = x_batch_cpu[row]   # (C,H,W)\n",
    "    x_rec  = x_hat_cpu[row]     # (C,H,W)\n",
    "    m_pix  = m_pix_cpu[row]     # (C,H,W)\n",
    "\n",
    "    img_orig = x_orig[band_idx].numpy()\n",
    "    img_rec  = x_rec[band_idx].numpy()\n",
    "    img_res  = img_rec - img_orig\n",
    "\n",
    "    mask_band = m_pix[band_idx].numpy()\n",
    "    valid_orig = img_orig[mask_band > 0]\n",
    "    valid_rec  = img_rec[mask_band > 0]\n",
    "    if valid_orig.size == 0 or valid_rec.size == 0:\n",
    "        valid_orig = img_orig\n",
    "        valid_rec  = img_rec\n",
    "\n",
    "    vmin_o, vmax_o = percentile_range(valid_orig, 1, 99)\n",
    "    vmin_r, vmax_r = percentile_range(valid_rec, 1, 99)\n",
    "    vmin = min(vmin_o, vmin_r)\n",
    "    vmax = max(vmax_o, vmax_r)\n",
    "\n",
    "    valid_res = img_res[mask_band > 0]\n",
    "    if valid_res.size == 0:\n",
    "        valid_res = img_res\n",
    "    res_amp = float(np.percentile(np.abs(valid_res.reshape(-1)), 99.0))\n",
    "    if res_amp == 0.0:\n",
    "        res_amp = 1.0\n",
    "\n",
    "    ax0, ax1, ax2 = axes[row]\n",
    "\n",
    "    im0 = ax0.imshow(img_orig, origin=\"lower\", cmap=\"gray\",\n",
    "                        vmin=vmin, vmax=vmax)\n",
    "    ax0.set_title(f\"Original ({BANDS[band_idx]}) idx={row}\")\n",
    "    fig.colorbar(im0, ax=ax0, fraction=0.046, pad=0.04)\n",
    "\n",
    "    im1 = ax1.imshow(img_rec, origin=\"lower\", cmap=\"gray\",\n",
    "                        vmin=vmin, vmax=vmax)\n",
    "    ax1.set_title(\"Reconstruction\")\n",
    "    fig.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "\n",
    "    im2 = ax2.imshow(img_res, origin=\"lower\", cmap=\"bwr\",\n",
    "                        vmin=-res_amp, vmax=res_amp)\n",
    "    ax2.set_title(\"Residual\")\n",
    "    fig.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "\n",
    "    for ax in (ax0, ax1, ax2):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "out_path = Path(\"inspect_plots\") / \"reconstruction_examples.png\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(out_path, dpi=150)\n",
    "plt.close(fig)\n",
    "print(\"Saved plot to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb5ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
