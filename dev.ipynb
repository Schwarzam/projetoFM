{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1dc8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdc1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767477ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astromodal.config import load_config\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import random\n",
    "from pathlib import Path\n",
    "from astromodal.datasets.datacubes import load_datacube_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a723eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"/home/schwarz/projetoFM/config.yaml\")\n",
    "\n",
    "hdd_folder = config['hdd_folder']\n",
    "\n",
    "hddfolder = Path(config[\"hdd_folder\"]) / \"image_latents\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2a1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "splus_bands = [\n",
    "    \"u\", \"i\", \"r\", \"g\", \"z\",\n",
    "    \"j0378\", \"j0395\", \"j0410\", \"j0430\",\n",
    "    \"j0515\", \"j0660\", \"j0861\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076cb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mandatory columns (must exist in all files)\n",
    "CORE_COLUMNS = [\"id\", \"ra\", \"dec\"]\n",
    "\n",
    "# your scalar features\n",
    "SCALAR_COLUMNS = [\n",
    "    \"ellipticity_det\",\n",
    "    \"elongation_det\",\n",
    "    \"a_pixel_det\",\n",
    "    \"b_pixel_det\",\n",
    "    \"theta_det\",\n",
    "    \"fwhm_n_det\",\n",
    "    *[f\"mag_pstotal_{b}\" for b in splus_bands],\n",
    "    *[f\"err_mag_pstotal_{b}\" for b in splus_bands],\n",
    "    \"gaia_parallax\",\n",
    "    \"gaia_parallax_error\",\n",
    "    \"gaia_pmra\",\n",
    "    \"gaia_pmdec\",\n",
    "    \"gaia_pmra_error\",\n",
    "    \"gaia_pmdec_error\",\n",
    "    \"gaia_phot_bp_mean_flux\",\n",
    "    \"gaia_phot_rp_mean_flux\",\n",
    "    \"gaia_phot_g_mean_flux\",\n",
    "    \"gaia_phot_bp_mean_flux_error\",\n",
    "    \"gaia_phot_rp_mean_flux_error\",\n",
    "    \"gaia_phot_g_mean_flux_error\",\n",
    "    \"gaia_teff_gspphot\",\n",
    "    \"gaia_logg_gspphot\",\n",
    "    \"gaia_mh_gspphot\",\n",
    "    \"specz_z\",\n",
    "    \"specz_e_z\",\n",
    "    \"vista_yapermag6\",\n",
    "    \"vista_yapermag6err\",\n",
    "    \"vista_japermag6\",\n",
    "    \"vista_japermag6err\",\n",
    "    \"vista_hapermag6\",\n",
    "    \"vista_hapermag6err\",\n",
    "    \"vista_ksapermag6\",\n",
    "    \"vista_ksapermag6err\",\n",
    "]\n",
    "\n",
    "# final schema (order matters for ML)\n",
    "EXPECTED_COLUMNS = CORE_COLUMNS + SCALAR_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b6c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import Sequence\n",
    "\n",
    "def read_parquet_with_schema(\n",
    "    path: str | Path,\n",
    "    *,\n",
    "    expected_columns: Sequence[str],\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a parquet file and guarantees that all expected_columns exist.\n",
    "    Missing columns are added as nulls.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "\n",
    "    # inspect schema without loading data\n",
    "    schema = pl.read_parquet_schema(path)\n",
    "    available = set(schema.keys())\n",
    "\n",
    "    # only read columns that exist\n",
    "    cols_to_read = [c for c in expected_columns if c in available]\n",
    "    df = pl.read_parquet(path, columns=cols_to_read, use_pyarrow=True)\n",
    "\n",
    "    # add missing columns as null\n",
    "    missing = [c for c in expected_columns if c not in df.columns]\n",
    "    if missing:\n",
    "        df = df.with_columns([pl.lit(None).alias(c) for c in missing])\n",
    "\n",
    "    # reorder to canonical schema\n",
    "    return df.select(expected_columns)\n",
    "\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from typing import Sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_datacubes_from_filelist(\n",
    "    files: Sequence[str | Path],\n",
    "    *,\n",
    "    expected_columns: Sequence[str],\n",
    "    desc: str = \"Loading datacubes\",\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load multiple parquet files given explicitly as a list.\n",
    "    Guarantees that ALL expected_columns exist in the final DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list[str | Path]\n",
    "        List of parquet files to read.\n",
    "    expected_columns : list[str]\n",
    "        Canonical schema (order is preserved).\n",
    "    desc : str\n",
    "        tqdm description.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Concatenated DataFrame with guaranteed schema.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "\n",
    "    for f in tqdm(files, desc=desc):\n",
    "        f = Path(f)\n",
    "        try:\n",
    "            # inspect schema first (cheap)\n",
    "            schema = pl.read_parquet_schema(f)\n",
    "            available = set(schema.keys())\n",
    "\n",
    "            cols_to_read = [c for c in expected_columns if c in available]\n",
    "            df = pl.read_parquet(f, columns=cols_to_read, use_pyarrow=True)\n",
    "\n",
    "            # add missing columns as null\n",
    "            missing = [c for c in expected_columns if c not in df.columns]\n",
    "            if missing:\n",
    "                df = df.with_columns([pl.lit(None).alias(c) for c in missing])\n",
    "\n",
    "            # enforce canonical order\n",
    "            df = df.select(expected_columns)\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {f}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"No valid parquet files were loaded.\")\n",
    "\n",
    "    return pl.concat(dfs, how=\"vertical\", rechunk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bff0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = config['datacubes_paths'].replace('*', 'STRIPE82-0002')\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "header = pl.read_parquet(file, n_rows=0)\n",
    "columns = [col for col in header.columns if 'gaiaxp' in col] + [\"id\", \"mag_psf_r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd01620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] - Found 2444 datacube files\n",
      "[info] - Subsampled to 500 files\n",
      "[info] - Training files: 450\n",
      "[info] - Validation files: 50\n"
     ]
    }
   ],
   "source": [
    "train_files, val_files = load_datacube_files(config['datacubes_paths'], train_val_split=0.9, nfiles_subsample=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab506bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datacubes:   0%|          | 0/450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datacubes: 100%|██████████| 450/450 [01:20<00:00,  5.58it/s]\n",
      "Loading datacubes: 100%|██████████| 50/50 [00:12<00:00,  3.88it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = load_datacubes_from_filelist(\n",
    "    train_files,\n",
    "    expected_columns=EXPECTED_COLUMNS,\n",
    ")\n",
    "\n",
    "train_df = train_df.filter(pl.col(\"mag_pstotal_r\") < 21)\n",
    "\n",
    "val_df = load_datacubes_from_filelist(\n",
    "    val_files,\n",
    "    expected_columns=EXPECTED_COLUMNS,\n",
    ")\n",
    "\n",
    "val_df = val_df.filter(pl.col(\"mag_pstotal_r\") < 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "862fb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_null_per_column(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a table with:\n",
    "    - column name\n",
    "    - number of non-null values\n",
    "    \"\"\"\n",
    "    return df.select([\n",
    "        pl.count().alias(\"n_rows\"),\n",
    "        *[\n",
    "            pl.col(c).count().alias(c)\n",
    "            for c in df.columns\n",
    "        ]\n",
    "    ]).transpose(\n",
    "        include_header=True,\n",
    "        header_name=\"column\",\n",
    "        column_names=[\"non_null_count\"],\n",
    "    ).filter(pl.col(\"column\") != \"n_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb87ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3667676/1653175618.py:8: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  pl.count().alias(\"n_rows\"),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (58, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column</th><th>non_null_count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;specz_e_z&quot;</td><td>299527</td></tr><tr><td>&quot;specz_z&quot;</td><td>360786</td></tr><tr><td>&quot;vista_hapermag6&quot;</td><td>1351706</td></tr><tr><td>&quot;vista_hapermag6err&quot;</td><td>1351706</td></tr><tr><td>&quot;gaia_teff_gspphot&quot;</td><td>2161068</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;b_pixel_det&quot;</td><td>7259195</td></tr><tr><td>&quot;theta_det&quot;</td><td>7259195</td></tr><tr><td>&quot;fwhm_n_det&quot;</td><td>7259195</td></tr><tr><td>&quot;mag_pstotal_r&quot;</td><td>7259195</td></tr><tr><td>&quot;err_mag_pstotal_r&quot;</td><td>7259195</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (58, 2)\n",
       "┌────────────────────┬────────────────┐\n",
       "│ column             ┆ non_null_count │\n",
       "│ ---                ┆ ---            │\n",
       "│ str                ┆ u32            │\n",
       "╞════════════════════╪════════════════╡\n",
       "│ specz_e_z          ┆ 299527         │\n",
       "│ specz_z            ┆ 360786         │\n",
       "│ vista_hapermag6    ┆ 1351706        │\n",
       "│ vista_hapermag6err ┆ 1351706        │\n",
       "│ gaia_teff_gspphot  ┆ 2161068        │\n",
       "│ …                  ┆ …              │\n",
       "│ b_pixel_det        ┆ 7259195        │\n",
       "│ theta_det          ┆ 7259195        │\n",
       "│ fwhm_n_det         ┆ 7259195        │\n",
       "│ mag_pstotal_r      ┆ 7259195        │\n",
       "│ err_mag_pstotal_r  ┆ 7259195        │\n",
       "└────────────────────┴────────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = count_non_null_per_column(train_df)\n",
    "nn.sort(\"non_null_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "830b404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_coverage(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    n = df.height\n",
    "    return pl.DataFrame({\n",
    "        \"column\": df.columns,\n",
    "        \"non_null\": [df.select(pl.col(c).count()).item() for c in df.columns],\n",
    "    }).with_columns(\n",
    "        (pl.col(\"non_null\") / n).alias(\"fraction\")\n",
    "    ).sort(\"fraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "284cb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = column_coverage(df)\n",
    "coverage.write_csv(\"column_coverage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc26d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astromodal.scalers.scaler1d import StandardScaler1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_pixel_det\n",
      "  train: mean=1.6234, std=0.8241\n",
      "  val:   mean=1.6207, std=0.7945\n",
      "theta_det\n",
      "  train: mean=-3.9161, std=52.2860\n",
      "  val:   mean=-0.4612, std=52.6731\n",
      "fwhm_n_det\n",
      "  train: mean=1.6279, std=1.8988\n",
      "  val:   mean=1.5703, std=1.8145\n",
      "mag_pstotal_u\n",
      "  train: mean=21.2817, std=2.0948\n",
      "  val:   mean=21.2497, std=2.0544\n",
      "mag_pstotal_i\n",
      "  train: mean=18.7422, std=1.7258\n",
      "  val:   mean=18.7187, std=1.7152\n",
      "mag_pstotal_r\n",
      "  train: mean=19.2012, std=1.7691\n",
      "  val:   mean=19.1764, std=1.7516\n",
      "mag_pstotal_g\n",
      "  train: mean=20.1524, std=1.9731\n",
      "  val:   mean=20.1448, std=1.9521\n",
      "mag_pstotal_z\n",
      "  train: mean=18.4973, std=1.7116\n",
      "  val:   mean=18.4764, std=1.7028\n",
      "mag_pstotal_j0378\n",
      "  train: mean=21.1070, std=2.0691\n",
      "  val:   mean=21.1009, std=2.0381\n",
      "mag_pstotal_j0395\n",
      "  train: mean=20.8347, std=2.0272\n",
      "  val:   mean=20.8651, std=2.0013\n",
      "mag_pstotal_j0410\n",
      "  train: mean=20.5400, std=2.1167\n",
      "  val:   mean=20.5864, std=2.0987\n",
      "mag_pstotal_j0430\n",
      "  train: mean=20.4869, std=2.1148\n",
      "  val:   mean=20.5491, std=2.0948\n",
      "mag_pstotal_j0515\n",
      "  train: mean=19.9515, std=2.0209\n",
      "  val:   mean=19.9591, std=1.9982\n",
      "mag_pstotal_j0660\n",
      "  train: mean=19.0777, std=1.8027\n",
      "  val:   mean=19.0710, std=1.8183\n",
      "mag_pstotal_j0861\n",
      "  train: mean=18.5766, std=1.7520\n",
      "  val:   mean=18.5496, std=1.7314\n",
      "err_mag_pstotal_u\n",
      "  train: mean=4299.5015, std=6789939.0000\n",
      "  val:   mean=396.2779, std=155774.3594\n",
      "err_mag_pstotal_i\n",
      "  train: mean=37.4980, std=25526.2129\n",
      "  val:   mean=3.0283, std=1905.9594\n",
      "err_mag_pstotal_r\n",
      "  train: mean=0.0762, std=3.3514\n",
      "  val:   mean=0.0709, std=1.0428\n",
      "err_mag_pstotal_g\n",
      "  train: mean=21.4587, std=22338.3945\n",
      "  val:   mean=0.3076, std=13.0554\n",
      "err_mag_pstotal_z\n",
      "  train: mean=28.7938, std=30067.2246\n",
      "  val:   mean=0.1572, std=12.4172\n",
      "err_mag_pstotal_j0378\n",
      "  train: mean=1710.4165, std=1064400.6250\n",
      "  val:   mean=420.4143, std=86565.4609\n",
      "err_mag_pstotal_j0395\n",
      "  train: mean=1066.1261, std=294518.6250\n",
      "  val:   mean=363.5547, std=63443.6719\n",
      "err_mag_pstotal_j0410\n",
      "  train: mean=2512.6460, std=2006498.0000\n",
      "  val:   mean=602.0776, std=146388.1406\n",
      "err_mag_pstotal_j0430\n",
      "  train: mean=1349.3733, std=792999.6250\n",
      "  val:   mean=9108.8555, std=7828408.0000\n",
      "err_mag_pstotal_j0515\n",
      "  train: mean=1512.9989, std=557962.5625\n",
      "  val:   mean=3335.4155, std=2640089.7500\n",
      "err_mag_pstotal_j0660\n",
      "  train: mean=1663.0049, std=711423.6250\n",
      "  val:   mean=5859.4321, std=4803577.5000\n",
      "err_mag_pstotal_j0861\n",
      "  train: mean=2017.9542, std=550668.5625\n",
      "  val:   mean=612.1005, std=91667.1328\n",
      "gaia_parallax\n",
      "  train: mean=0.6696, std=0.9919\n",
      "  val:   mean=0.6576, std=0.9880\n",
      "gaia_parallax_error\n",
      "  train: mean=0.3731, std=0.3729\n",
      "  val:   mean=0.3863, std=0.3820\n",
      "gaia_pmra\n",
      "  train: mean=-3.1761, std=10.1422\n",
      "  val:   mean=1.3111, std=8.5110\n",
      "gaia_pmdec\n",
      "  train: mean=-2.6413, std=8.1281\n",
      "  val:   mean=-5.0899, std=7.7729\n",
      "gaia_pmra_error\n",
      "  train: mean=0.3831, std=0.3995\n",
      "  val:   mean=0.4284, std=0.4370\n",
      "gaia_pmdec_error\n",
      "  train: mean=0.3643, std=0.3842\n",
      "  val:   mean=0.3625, std=0.3744\n",
      "gaia_phot_bp_mean_flux\n",
      "  train: mean=3153.9591, std=51729.8740\n",
      "  val:   mean=2763.3496, std=19978.0318\n",
      "gaia_phot_rp_mean_flux\n",
      "  train: mean=4729.4292, std=76371.1006\n",
      "  val:   mean=4289.2633, std=31727.1638\n",
      "gaia_phot_g_mean_flux\n",
      "  train: mean=6361.7174, std=100480.6294\n",
      "  val:   mean=5677.8405, std=40923.0171\n",
      "gaia_phot_bp_mean_flux_error\n",
      "  train: mean=11.8106, std=212.5980\n",
      "  val:   mean=11.2499, std=55.9629\n",
      "gaia_phot_rp_mean_flux_error\n",
      "  train: mean=13.0167, std=332.3019\n",
      "  val:   mean=12.6484, std=103.6379\n",
      "gaia_phot_g_mean_flux_error\n",
      "  train: mean=3.3797, std=65.6254\n",
      "  val:   mean=3.2542, std=45.7296\n",
      "gaia_teff_gspphot\n",
      "  train: mean=4843.7092, std=842.8037\n",
      "  val:   mean=4796.5475, std=796.1232\n",
      "gaia_logg_gspphot\n",
      "  train: mean=4.5643, std=0.3147\n",
      "  val:   mean=4.5721, std=0.3207\n",
      "gaia_mh_gspphot\n",
      "  train: mean=-0.8293, std=0.6928\n",
      "  val:   mean=-0.8854, std=0.7157\n",
      "specz_z\n",
      "  train: mean=0.3368, std=0.6800\n",
      "  val:   mean=0.2254, std=0.4252\n",
      "specz_e_z\n",
      "  train: mean=0.0006, std=0.1438\n",
      "  val:   mean=0.0063, std=1.1364\n",
      "vista_yapermag6\n",
      "  train: mean=17.6310, std=1.6657\n",
      "  val:   mean=nan, std=nan\n",
      "vista_yapermag6err\n",
      "  train: mean=0.0779, std=0.8571\n",
      "  val:   mean=nan, std=nan\n",
      "vista_japermag6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3667676/559473229.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  val_mean = val_values.mean()\n",
      "/home/schwarz/miniconda3/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/schwarz/miniconda3/lib/python3.13/site-packages/numpy/_core/_methods.py:222: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/schwarz/miniconda3/lib/python3.13/site-packages/numpy/_core/_methods.py:180: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/schwarz/miniconda3/lib/python3.13/site-packages/numpy/_core/_methods.py:214: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train: mean=17.1637, std=1.6277\n",
      "  val:   mean=17.2050, std=1.6329\n",
      "vista_japermag6err\n",
      "  train: mean=0.0943, std=7.1069\n",
      "  val:   mean=0.0827, std=0.8039\n",
      "vista_hapermag6\n",
      "  train: mean=16.5750, std=1.5731\n",
      "  val:   mean=16.5888, std=1.6005\n",
      "vista_hapermag6err\n",
      "  train: mean=0.1701, std=49.8035\n",
      "  val:   mean=0.1007, std=0.6168\n",
      "vista_ksapermag6\n",
      "  train: mean=16.1005, std=1.4546\n",
      "  val:   mean=16.2140, std=1.4916\n",
      "vista_ksapermag6err\n",
      "  train: mean=0.1702, std=4.3447\n",
      "  val:   mean=0.1676, std=10.1178\n"
     ]
    }
   ],
   "source": [
    "from astromodal.scalers.scaler1d import StandardScaler1D\n",
    "from astromodal.tokenizers.rvq import ResidualVQ\n",
    "from astromodal.tokenizers.spectralrvq import SpectralResidualVQ\n",
    "\n",
    "for col in SCALAR_COLUMNS[3:]:\n",
    "    print(col)\n",
    "    values = train_df.select(pl.col(col).drop_nulls()).to_series().to_numpy()\n",
    "\n",
    "    mean = values.mean()\n",
    "    std = values.std()\n",
    "\n",
    "    scaler = StandardScaler1D(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "    )\n",
    "\n",
    "    val_values = val_df.select(pl.col(col).drop_nulls()).to_series().to_numpy()\n",
    "    val_mean = val_values.mean()\n",
    "    val_std = val_values.std()\n",
    "\n",
    "    print(f\"  train: mean={mean:.4f}, std={std:.4f}\")\n",
    "    print(f\"  val:   mean={val_mean:.4f}, std={val_std:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4d6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit + save (1) a StandardScaler1D and (2) a 1D SpectralPatchRVQ tokenizer\n",
    "for EACH scalar column (including error columns), handling missing columns\n",
    "gracefully.\n",
    "\n",
    "Notes:\n",
    "- For scalar columns we treat each object value as a length-1 sequence: [B, L=1, C=1].\n",
    "- The tokenizer is therefore RVQ(dim=1) + SpectralPatchRVQ(patch_size=1, channels=1).\n",
    "- If a column is missing (or has too few finite values), we still save a \"default\"\n",
    "  scaler (mean=0,std=1) and *skip* tokenizer training (or you can train anyway on empties).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from astromodal.scalers.scaler1d import StandardScaler1D\n",
    "from astromodal.tokenizers.rvq import ResidualVQ\n",
    "from astromodal.tokenizers.spectralrvq import SpectralPatchRVQ\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# config / paths\n",
    "# -------------------------\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE_T = torch.float32\n",
    "\n",
    "OUT_ROOT = Path(\"outputs/scalars_tokenizers\")  # <-- change to your config[\"models_folder\"]/...\n",
    "SCALERS_DIR = OUT_ROOT / \"scalers\"\n",
    "TOK_DIR = OUT_ROOT / \"tokenizers\"\n",
    "SCALERS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TOK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# RVQ/tokenizer hyperparams per scalar column\n",
    "CODEBOOK_SIZE = 1024\n",
    "NUM_STAGES = 3\n",
    "DECAY = 0.99\n",
    "\n",
    "# training\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 4096\n",
    "EMA_EPOCHS = 10  # freeze EMA after this (stability)\n",
    "MIN_FINITE = 200  # skip training if too few values\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# columns\n",
    "# -------------------------\n",
    "\n",
    "CORE_COLUMNS = [\"id\", \"ra\", \"dec\"]\n",
    "\n",
    "# you already define splus_bands somewhere above\n",
    "# splus_bands = [...]\n",
    "\n",
    "SCALAR_COLUMNS = [\n",
    "    \"ellipticity_det\",\n",
    "    \"elongation_det\",\n",
    "    \"a_pixel_det\",\n",
    "    \"b_pixel_det\",\n",
    "    \"theta_det\",\n",
    "    \"fwhm_n_det\",\n",
    "    *[f\"mag_pstotal_{b}\" for b in splus_bands],\n",
    "    *[f\"err_mag_pstotal_{b}\" for b in splus_bands],\n",
    "    \"gaia_parallax\",\n",
    "    \"gaia_parallax_error\",\n",
    "    \"gaia_pmra\",\n",
    "    \"gaia_pmdec\",\n",
    "    \"gaia_pmra_error\",\n",
    "    \"gaia_pmdec_error\",\n",
    "    \"gaia_phot_bp_mean_flux\",\n",
    "    \"gaia_phot_rp_mean_flux\",\n",
    "    \"gaia_phot_g_mean_flux\",\n",
    "    \"gaia_phot_bp_mean_flux_error\",\n",
    "    \"gaia_phot_rp_mean_flux_error\",\n",
    "    \"gaia_phot_g_mean_flux_error\",\n",
    "    \"gaia_teff_gspphot\",\n",
    "    \"gaia_logg_gspphot\",\n",
    "    \"gaia_mh_gspphot\",\n",
    "    \"specz_z\",\n",
    "    \"specz_e_z\",\n",
    "    \"vista_yapermag6\",\n",
    "    \"vista_yapermag6err\",\n",
    "    \"vista_japermag6\",\n",
    "    \"vista_japermag6err\",\n",
    "    \"vista_hapermag6\",\n",
    "    \"vista_hapermag6err\",\n",
    "    \"vista_ksapermag6\",\n",
    "    \"vista_ksapermag6err\",\n",
    "]\n",
    "\n",
    "EXPECTED_COLUMNS = CORE_COLUMNS + SCALAR_COLUMNS\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# helpers\n",
    "# -------------------------\n",
    "\n",
    "def _finite_values_from_df(df: pl.DataFrame, col: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns finite float64 values from df[col], empty if col missing.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "    s = df.select(pl.col(col)).to_series()\n",
    "    # Convert to numpy + filter finite\n",
    "    v = s.to_numpy()\n",
    "    v = v.astype(np.float64, copy=False)\n",
    "    v = v[np.isfinite(v)]\n",
    "    return v\n",
    "\n",
    "\n",
    "def fit_scaler_1d(\n",
    "    train_df: pl.DataFrame,\n",
    "    col: str,\n",
    "    *,\n",
    "    transform: str = \"none\",      # optionally \"asinh\"\n",
    "    asinh_scale: float = 1.0,\n",
    "    clip_quantile: Optional[float] = None,  # e.g. 0.999\n",
    ") -> StandardScaler1D:\n",
    "    \"\"\"\n",
    "    Fit StandardScaler1D pooling all finite values from the column.\n",
    "    \"\"\"\n",
    "    v = _finite_values_from_df(train_df, col)\n",
    "    if v.size == 0:\n",
    "        return StandardScaler1D(mean=0.0, std=1.0, transform=transform, asinh_scale=asinh_scale)\n",
    "\n",
    "    if transform == \"asinh\":\n",
    "        s0 = asinh_scale if asinh_scale > 0 else 1.0\n",
    "        v = np.arcsinh(v / s0)\n",
    "\n",
    "    if clip_quantile is not None and 0.0 < clip_quantile < 1.0 and v.size > 10:\n",
    "        lo = np.quantile(v, 1.0 - clip_quantile)\n",
    "        hi = np.quantile(v, clip_quantile)\n",
    "        v = np.clip(v, lo, hi)\n",
    "\n",
    "    mean = float(np.mean(v))\n",
    "    std = float(np.std(v))\n",
    "    if not np.isfinite(std) or std < 1e-12:\n",
    "        std = 1.0\n",
    "\n",
    "    return StandardScaler1D(mean=mean, std=std, transform=transform, asinh_scale=asinh_scale)\n",
    "\n",
    "\n",
    "def make_scalar_loader(\n",
    "    df: pl.DataFrame,\n",
    "    col: str,\n",
    "    scaler: StandardScaler1D,\n",
    "    *,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Returns DataLoader yielding x_norm: [B, L=1, C=1].\n",
    "    Missing/NaN values are dropped.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        # empty loader\n",
    "        Xn = np.zeros((0, 1, 1), dtype=np.float32)\n",
    "        return DataLoader(TensorDataset(torch.from_numpy(Xn)), batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # take finite values only\n",
    "    v = _finite_values_from_df(df, col).astype(np.float32)  # [N]\n",
    "    if v.size == 0:\n",
    "        Xn = np.zeros((0, 1, 1), dtype=np.float32)\n",
    "        return DataLoader(TensorDataset(torch.from_numpy(Xn)), batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # normalize\n",
    "    vn = scaler.transform_x(v).astype(np.float32)  # StandardScaler1D works elementwise\n",
    "    Xn = vn.reshape(-1, 1, 1)  # [N,1,1]\n",
    "\n",
    "    ds = TensorDataset(torch.from_numpy(Xn))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_scalar_tok_unscaled_mse(\n",
    "    tok: SpectralPatchRVQ,\n",
    "    dl: DataLoader,\n",
    "    scaler: StandardScaler1D,\n",
    "    *,\n",
    "    device: str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate unscaled MSE/RMSE in original units.\n",
    "    Each batch x_norm is [B,1,1].\n",
    "    \"\"\"\n",
    "    tok.eval().to(device)\n",
    "\n",
    "    sum_mse = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for (x_norm,) in dl:\n",
    "        if x_norm.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        x_norm = x_norm.to(device=device, dtype=DTYPE_T, non_blocking=True)  # [B,1,1]\n",
    "        out = tok.encode(x_norm, update_ema=False)\n",
    "        xq_norm = out[\"x_q\"]  # [B,1,1]\n",
    "\n",
    "        x_raw = scaler.inverse_transform_x(x_norm.detach().cpu().numpy().reshape(-1))   # [B]\n",
    "        xq_raw = scaler.inverse_transform_x(xq_norm.detach().cpu().numpy().reshape(-1)) # [B]\n",
    "\n",
    "        mse = float(np.mean((xq_raw - x_raw) ** 2))\n",
    "        sum_mse += mse * x_norm.shape[0]\n",
    "        n += x_norm.shape[0]\n",
    "\n",
    "    mse = sum_mse / max(n, 1)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    return {\n",
    "        \"mse\": float(mse),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"mse_x_1e6\": float(mse * 1e6),\n",
    "        \"-log10(mse)\": float(-np.log10(mse + 1e-300)),\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_raw_metrics(x_norm, xq_norm, scaler):\n",
    "    x_raw  = scaler.inverse_transform_x(x_norm)\n",
    "    xq_raw = scaler.inverse_transform_x(xq_norm)\n",
    "    diff2 = (xq_raw - x_raw) ** 2\n",
    "    mse = float(diff2.mean())\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    return mse, rmse\n",
    "\n",
    "def train_scalar_tokenizer(\n",
    "    train_dl: DataLoader,\n",
    "    val_dl: DataLoader,\n",
    "    *,\n",
    "    codebook_size: int,\n",
    "    num_stages: int,\n",
    "    decay: float,\n",
    "    epochs: int,\n",
    "    ema_epochs: int,\n",
    "    device: str,\n",
    "    save_path: Path,\n",
    "    scaler: StandardScaler1D,\n",
    ") -> Tuple[SpectralPatchRVQ, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Train a scalar tokenizer (RVQ(dim=1) + SpectralPatchRVQ(patch_size=1, channels=1))\n",
    "    and save best checkpoint by val(unscaled) MSE.\n",
    "    \"\"\"\n",
    "    P = 1\n",
    "    C = 1\n",
    "    rvq = ResidualVQ(dim=P * C, num_stages=num_stages, codebook_size=codebook_size, decay=decay).to(device)\n",
    "    tok = SpectralPatchRVQ(rvq=rvq, patch_size=P, channels=C).to(device)\n",
    "\n",
    "    best_mse = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "    history: List[Dict[str, Any]] = []\n",
    "\n",
    "    tok.train()\n",
    "    for epoch in range(epochs):\n",
    "        update_ema = (epoch < int(ema_epochs))\n",
    "        tr = tok.train_epoch(train_dl, device=device, update_ema=update_ema)\n",
    "\n",
    "        va = eval_scalar_tok_unscaled_mse(tok, val_dl, scaler, device=device)\n",
    "        row = {\"epoch\": epoch, \"train_loss_norm\": float(tr[\"loss\"]), \"update_ema\": bool(update_ema), **va}\n",
    "        history.append(row)\n",
    "\n",
    "        if va[\"mse\"] < best_mse:\n",
    "            best_mse = va[\"mse\"]\n",
    "            best_epoch = epoch\n",
    "            tok.save(\n",
    "                str(save_path),\n",
    "                additional_info={\n",
    "                    \"best_epoch\": int(best_epoch),\n",
    "                    \"best_val_mse\": float(best_mse),\n",
    "                    \"codebook_size\": int(codebook_size),\n",
    "                    \"num_stages\": int(num_stages),\n",
    "                    \"decay\": float(decay),\n",
    "                    \"patch_size\": int(P),\n",
    "                    \"channels\": int(C),\n",
    "                },\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | ema={int(update_ema)} | \"\n",
    "            f\"train_loss(norm)={tr['loss']:.6g} | \"\n",
    "            f\"val_mse={va['mse']:.6g} rmse={va['rmse']:.6g} | \"\n",
    "            f\"best_mse={best_mse:.6g} @ {best_epoch}\"\n",
    "        )\n",
    "\n",
    "    return tok, history\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# main: per-column fit + save scaler + tokenizer\n",
    "# -------------------------\n",
    "\n",
    "def fit_and_save_all_scalars(\n",
    "    train_df: pl.DataFrame,\n",
    "    val_df: pl.DataFrame,\n",
    "    *,\n",
    "    scalar_columns: List[str],\n",
    "    scalers_dir: Path,\n",
    "    tok_dir: Path,\n",
    "    transform_for_errors: str = \"asinh\",  # often good for error-like heavy tails\n",
    "    transform_for_values: str = \"none\",\n",
    "    asinh_scale_default: float = 1.0,\n",
    "    clip_quantile: Optional[float] = 0.999,\n",
    "):\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    for col in tqdm(scalar_columns, desc=\"Fitting scalers + tokenizers\"):\n",
    "        is_error_col = (\"err_\" in col) or col.endswith(\"error\") or col.endswith(\"err\")\n",
    "\n",
    "        # choose scaler options\n",
    "        transform = transform_for_errors if is_error_col else transform_for_values\n",
    "        asinh_scale = asinh_scale_default\n",
    "\n",
    "        # ---- fit scaler (train) ----\n",
    "        scaler = fit_scaler_1d(\n",
    "            train_df,\n",
    "            col,\n",
    "            transform=transform,\n",
    "            asinh_scale=asinh_scale,\n",
    "            clip_quantile=clip_quantile,\n",
    "        )\n",
    "\n",
    "        # ---- report train/val stats (raw, without transform) ----\n",
    "        tr_vals = _finite_values_from_df(train_df, col)\n",
    "        va_vals = _finite_values_from_df(val_df, col)\n",
    "\n",
    "        tr_mean = float(np.mean(tr_vals)) if tr_vals.size else float(\"nan\")\n",
    "        tr_std = float(np.std(tr_vals)) if tr_vals.size else float(\"nan\")\n",
    "        va_mean = float(np.mean(va_vals)) if va_vals.size else float(\"nan\")\n",
    "        va_std = float(np.std(va_vals)) if va_vals.size else float(\"nan\")\n",
    "\n",
    "        print(f\"\\n[{col}]\")\n",
    "        print(f\"  train(raw): mean={tr_mean:.6g}, std={tr_std:.6g}, n={tr_vals.size}\")\n",
    "        print(f\"  val(raw):   mean={va_mean:.6g}, std={va_std:.6g}, n={va_vals.size}\")\n",
    "        print(f\"  scaler: mean={scaler.mean:.6g}, std={scaler.std:.6g}, transform={scaler.transform}\")\n",
    "\n",
    "        # ---- save scaler ----\n",
    "        scaler_path = scalers_dir / f\"{col}.npz\"\n",
    "        scaler.save(scaler_path)\n",
    "\n",
    "        # ---- if missing or too few points, skip tokenizer training but still record ----\n",
    "        if tr_vals.size < MIN_FINITE or col not in train_df.columns:\n",
    "            print(f\"  -> skip tokenizer (too few finite values or missing). saved scaler only.\")\n",
    "            results.append({\n",
    "                \"col\": col,\n",
    "                \"saved_scaler\": str(scaler_path),\n",
    "                \"saved_tokenizer\": None,\n",
    "                \"train_n_finite\": int(tr_vals.size),\n",
    "                \"val_n_finite\": int(va_vals.size),\n",
    "                \"skipped_tokenizer\": True,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # ---- train tokenizer for this scalar ----\n",
    "        train_dl = make_scalar_loader(train_df, col, scaler, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_dl = make_scalar_loader(val_df, col, scaler, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        tok_path = tok_dir / f\"{col}.pt\"\n",
    "        tok, hist = train_scalar_tokenizer(\n",
    "            train_dl,\n",
    "            val_dl,\n",
    "            codebook_size=CODEBOOK_SIZE,\n",
    "            num_stages=NUM_STAGES,\n",
    "            decay=DECAY,\n",
    "            epochs=EPOCHS,\n",
    "            ema_epochs=EMA_EPOCHS,\n",
    "            device=DEVICE,\n",
    "            save_path=tok_path,\n",
    "            scaler=scaler,\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"col\": col,\n",
    "            \"saved_scaler\": str(scaler_path),\n",
    "            \"saved_tokenizer\": str(tok_path),\n",
    "            \"train_n_finite\": int(tr_vals.size),\n",
    "            \"val_n_finite\": int(va_vals.size),\n",
    "            \"skipped_tokenizer\": False,\n",
    "            \"best_val_mse\": float(min(h[\"mse\"] for h in hist)) if hist else None,\n",
    "        })\n",
    "\n",
    "    return pl.DataFrame(results)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# usage\n",
    "# -------------------------\n",
    "# train_df, val_df are your polars dataframes\n",
    "# Make sure they already have the scalar columns as numeric (Float32/Float64).\n",
    "#\n",
    "# out_df = fit_and_save_all_scalars(train_df, val_df, scalar_columns=SCALAR_COLUMNS,\n",
    "#                                  scalers_dir=SCALERS_DIR, tok_dir=TOK_DIR)\n",
    "# out_df.write_parquet(OUT_ROOT / \"scalar_fit_summary.parquet\")\n",
    "# print(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c87e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scalers + tokenizers:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ellipticity_det]\n",
      "  train(raw): mean=0.159833, std=0.125802, n=7259195\n",
      "  val(raw):   mean=0.156818, std=0.125913, n=905803\n",
      "  scaler: mean=0.159777, std=0.125484, transform=none\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 000 | ema=1 | train_loss(norm)=4.13459e-07 | val_mse=1.09266e-09 rmse=3.30555e-05 | best_mse=1.09266e-09 @ 0\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 001 | ema=1 | train_loss(norm)=3.64264e-08 | val_mse=3.89674e-10 rmse=1.97402e-05 | best_mse=3.89674e-10 @ 1\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 002 | ema=1 | train_loss(norm)=1.52262e-08 | val_mse=1.77457e-10 rmse=1.33213e-05 | best_mse=1.77457e-10 @ 2\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 003 | ema=1 | train_loss(norm)=8.32845e-09 | val_mse=9.7587e-11 rmse=9.87862e-06 | best_mse=9.7587e-11 @ 3\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 004 | ema=1 | train_loss(norm)=5.3494e-09 | val_mse=7.32528e-11 rmse=8.55878e-06 | best_mse=7.32528e-11 @ 4\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 005 | ema=1 | train_loss(norm)=3.79669e-09 | val_mse=5.54665e-11 rmse=7.44758e-06 | best_mse=5.54665e-11 @ 5\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 006 | ema=1 | train_loss(norm)=2.93524e-09 | val_mse=3.87816e-11 rmse=6.22748e-06 | best_mse=3.87816e-11 @ 6\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 007 | ema=1 | train_loss(norm)=2.18472e-09 | val_mse=3.6028e-11 rmse=6.00233e-06 | best_mse=3.6028e-11 @ 7\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 008 | ema=1 | train_loss(norm)=1.7078e-09 | val_mse=2.44946e-11 rmse=4.9492e-06 | best_mse=2.44946e-11 @ 8\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/ellipticity_det.pt\n",
      "Epoch 009 | ema=1 | train_loss(norm)=1.40962e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 010 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 011 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 012 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 013 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 014 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 015 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 016 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 017 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 018 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 019 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 020 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 021 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 022 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 023 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 024 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 025 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 026 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 027 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "Epoch 028 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scalers + tokenizers:   2%|▏         | 1/55 [08:50<7:57:35, 530.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 | ema=0 | train_loss(norm)=1.27468e-09 | val_mse=2.1642e-11 rmse=4.6521e-06 | best_mse=2.1642e-11 @ 9\n",
      "\n",
      "[elongation_det]\n",
      "  train(raw): mean=1.24178, std=9.76633, n=7259195\n",
      "  val(raw):   mean=1.23485, std=1.53701, n=905803\n",
      "  scaler: mean=1.23145, std=0.305562, transform=none\n",
      "[info] - Saved SpectralPatchRVQ to outputs/scalars_tokenizers/tokenizers/elongation_det.pt\n",
      "Epoch 000 | ema=1 | train_loss(norm)=742.255 | val_mse=0.1359 rmse=0.368646 | best_mse=0.1359 @ 0\n",
      "Epoch 001 | ema=1 | train_loss(norm)=835.897 | val_mse=0.151534 rmse=0.389273 | best_mse=0.1359 @ 0\n"
     ]
    }
   ],
   "source": [
    "out_df = fit_and_save_all_scalars(train_df, val_df, scalar_columns=SCALAR_COLUMNS,\n",
    "                                 scalers_dir=SCALERS_DIR, tok_dir=TOK_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60ee97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
