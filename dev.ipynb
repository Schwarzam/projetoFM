{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dc8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767477ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astromodal.config import load_config\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca31f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/astrodados4/downloads/hypercube/datacube_*.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'datacubes_paths': '/home/astrodados4/downloads/hypercube/datacube_*.parquet',\n",
       " 'models_folder': '/home/schwarz/projetoFM/models',\n",
       " 'hdd_folder': '/home/astrodados4/downloads/projetin'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config(\"config.yaml\")\n",
    "\n",
    "print(config['datacubes_paths'])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a723eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdd_folder = config['hdd_folder']\n",
    "\n",
    "hddfolder = Path(config[\"hdd_folder\"]) / \"image_latents\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529c26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_latents = None\n",
    "for key, file in enumerate(hddfolder.glob(\"*.parquet\")):\n",
    "    latents = pl.read_parquet(file, use_pyarrow=True)\n",
    "    \n",
    "    if all_latents is None:\n",
    "        all_latents = latents\n",
    "    else:\n",
    "        all_latents = pl.concat([all_latents, latents])\n",
    "        \n",
    "    if key == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3615fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astromodal.tokenizers.resvq import ResidualVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7225a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm \n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "latents_tensor = torch.tensor(all_latents['latent'].to_numpy(), device=device)\n",
    "\n",
    "D = latents_tensor.shape[1]\n",
    "latents = F.normalize(latents_tensor, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec60fad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean ||z||: 1.0\n",
      "std  ||z||: 5.756724519301315e-08\n",
      "mean z: -0.02639772742986679 std z: 0.01308493409305811\n"
     ]
    }
   ],
   "source": [
    "print(\"mean ||z||:\", latents.norm(dim=1).mean().item())\n",
    "print(\"std  ||z||:\", latents.norm(dim=1).std().item())\n",
    "print(\"mean z:\", latents.mean().item(), \"std z:\", latents.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56de785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8258b145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | mean_sq_L2=0.114141 | per_dim_MSE=0.00009908 | dead_codes_per_stage=[451, 0, 0, 0, 0, 0]\n",
      "0 61 3.2726593017578125 9.664260223656525\n",
      "1 512 6.732638359069824 106.34720884607334\n",
      "2 512 6.44869327545166 87.3474254486766\n",
      "3 512 6.518250465393066 91.66191142330456\n",
      "4 512 6.487922191619873 89.75511117874211\n",
      "5 512 6.415622711181641 85.36795479574738\n",
      "Epoch 01 | mean_sq_L2=0.101388 | per_dim_MSE=0.00008801 | dead_codes_per_stage=[441, 7, 17, 27, 34, 36]\n",
      "0 71 3.390507221221924 10.486833532841914\n",
      "1 505 6.947203159332275 123.40039132633322\n",
      "2 495 6.4447197914123535 87.10718335114065\n",
      "3 485 6.431288242340088 86.29997535764055\n",
      "4 478 6.421722888946533 85.72968212402763\n",
      "5 476 6.375598907470703 83.03219358363746\n",
      "Epoch 02 | mean_sq_L2=0.100963 | per_dim_MSE=0.00008764 | dead_codes_per_stage=[433, 2, 1, 2, 6, 5]\n",
      "0 79 3.3804008960723877 10.41362818033081\n",
      "1 510 6.880362033843994 117.81358003317513\n",
      "2 511 6.398797512054443 84.37814769017976\n",
      "3 510 6.381698131561279 83.38396853416555\n",
      "4 506 6.379439353942871 83.25351929148717\n",
      "5 507 6.369447708129883 82.67892399300078\n",
      "Epoch 03 | mean_sq_L2=0.100714 | per_dim_MSE=0.00008743 | dead_codes_per_stage=[430, 1, 1, 2, 2, 1]\n",
      "0 82 3.3974409103393555 10.537355266247731\n",
      "1 511 6.83598518371582 114.24483906337298\n",
      "2 511 6.365376949310303 82.44596306733403\n",
      "3 510 6.37327241897583 82.89840385978657\n",
      "4 510 6.381350040435791 83.36385221204978\n",
      "5 511 6.3641133308410645 82.37378244173297\n",
      "Epoch 04 | mean_sq_L2=0.100514 | per_dim_MSE=0.00008725 | dead_codes_per_stage=[429, 1, 3, 1, 1, 2]\n",
      "0 83 3.4584403038024902 10.99244419828455\n",
      "1 511 6.81444787979126 112.55199976083789\n",
      "2 509 6.358719825744629 82.06640356983031\n",
      "3 511 6.382850646972656 83.4506074995581\n",
      "4 511 6.394331455230713 84.11734764774002\n",
      "5 510 6.346949577331543 81.39958743939057\n",
      "Epoch 05 | mean_sq_L2=0.100319 | per_dim_MSE=0.00008708 | dead_codes_per_stage=[426, 1, 0, 3, 1, 2]\n",
      "0 86 3.4911859035491943 11.2447984964035\n",
      "1 511 6.79749870300293 111.23744501124439\n",
      "2 512 6.363842964172363 82.35834672053167\n",
      "3 509 6.377739906311035 83.15550710418931\n",
      "4 511 6.404098033905029 84.68872672059602\n",
      "5 510 6.3709025382995605 82.7623404169696\n",
      "Epoch 06 | mean_sq_L2=0.100148 | per_dim_MSE=0.00008693 | dead_codes_per_stage=[424, 0, 0, 1, 1, 1]\n",
      "0 88 3.5046136379241943 11.349946861145924\n",
      "1 512 6.785638809204102 110.32674907799019\n",
      "2 512 6.360352516174316 82.15933026179191\n",
      "3 511 6.4047136306762695 84.72487104051388\n",
      "4 511 6.4120025634765625 85.15400993634148\n",
      "5 511 6.420764446258545 85.67274722288329\n",
      "Epoch 07 | mean_sq_L2=0.100032 | per_dim_MSE=0.00008683 | dead_codes_per_stage=[423, 0, 0, 3, 2, 2]\n",
      "0 89 3.501392126083374 11.324630911358408\n",
      "1 512 6.782811164855957 110.1107233221147\n",
      "2 512 6.352987289428711 81.74096042322255\n",
      "3 509 6.442094326019287 86.94880689604096\n",
      "4 510 6.4254961013793945 85.95419227405253\n",
      "5 510 6.446691989898682 87.22634237375573\n",
      "Epoch 08 | mean_sq_L2=0.099848 | per_dim_MSE=0.00008667 | dead_codes_per_stage=[422, 0, 0, 1, 0, 2]\n",
      "0 90 3.512523889541626 11.412349182117342\n",
      "1 512 6.787628173828125 110.47898604983557\n",
      "2 512 6.371918678283691 82.82065332749862\n",
      "3 511 6.455268383026123 87.74642137405512\n",
      "4 512 6.425987243652344 85.98345897478656\n",
      "5 510 6.469624996185303 88.62396668582294\n",
      "Epoch 09 | mean_sq_L2=0.099712 | per_dim_MSE=0.00008656 | dead_codes_per_stage=[419, 0, 3, 1, 4, 1]\n",
      "0 93 3.521190643310547 11.48111333540119\n",
      "1 512 6.78619384765625 110.36920251908037\n",
      "2 509 6.393506050109863 84.06923558686131\n",
      "3 511 6.464845180511475 88.33083108948863\n",
      "4 508 6.441882610321045 86.93604808271725\n",
      "5 511 6.486093044281006 89.64138562639882\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "loader = DataLoader(TensorDataset(latents), batch_size=2048, shuffle=True, drop_last=True)\n",
    "\n",
    "rvq = ResidualVQ(dim=D, num_stages=6, codebook_size=512, decay=0.99).to(device)\n",
    "    \n",
    "rvq.train()\n",
    "\n",
    "def recon_errors(z, z_hat):\n",
    "    # mean squared L2 error: E ||z - z_hat||^2\n",
    "    mean_l2_sq = ((z - z_hat) ** 2).sum(dim=1).mean()\n",
    "    # per-dim MSE: mean over all elements\n",
    "    mse = ((z - z_hat) ** 2).mean()\n",
    "    return mean_l2_sq.item(), mse.item()\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Running sums for epoch metrics\n",
    "    n_sum = 0\n",
    "    mse_sum = 0.0\n",
    "    l2_sum = 0.0\n",
    "\n",
    "    # Correct dead-code tracking across the whole epoch\n",
    "    counts = torch.zeros(rvq.R, rvq.K, device=device)\n",
    "\n",
    "    for (x,) in loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        out = rvq(x, update_ema=True)\n",
    "        z_q = out[\"z_q\"]\n",
    "        codes = out[\"codes\"]   # [B, R]\n",
    "\n",
    "        # recon metrics (batch)\n",
    "        mean_l2_sq, mse = recon_errors(x, z_q)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        n_sum += bs\n",
    "        mse_sum += mse * bs\n",
    "        l2_sum += mean_l2_sq * bs\n",
    "\n",
    "        # accumulate token usage counts per stage\n",
    "        for s in range(rvq.R):\n",
    "            counts[s].scatter_add_(\n",
    "                0,\n",
    "                codes[:, s],\n",
    "                torch.ones_like(codes[:, s], dtype=counts.dtype)\n",
    "            )\n",
    "\n",
    "    epoch_mse = mse_sum / n_sum           # per-dim MSE\n",
    "    epoch_l2 = l2_sum / n_sum             # mean squared L2\n",
    "\n",
    "    dead = (counts == 0).sum(dim=1).tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dead0 = (counts[0] == 0).nonzero(as_tuple=False).squeeze(1)\n",
    "        if dead0.numel() > 0:\n",
    "            # reseed dead codes from random data points\n",
    "            ridx = torch.randint(0, latents.shape[0], (dead0.numel(),), device=device)\n",
    "            rvq.embed[0, dead0].copy_(latents[ridx].to(device))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        e = rvq.embed[0]              # [K, D]\n",
    "        rvq.embed[0].copy_(e / (e.norm(dim=1, keepdim=True) + 1e-8))\n",
    "        \n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | mean_sq_L2={epoch_l2:.6f} | per_dim_MSE={epoch_mse:.8f} \"\n",
    "        f\"| dead_codes_per_stage={dead}\"\n",
    "    )\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     c0 = counts[0]\n",
    "    #     used0 = (c0 > 0).sum().item()\n",
    "    #     p = c0 / c0.sum()\n",
    "    #     p = p[p > 0]\n",
    "    #     entropy_bits = (-(p * p.log()).sum() / torch.log(torch.tensor(2.0, device=device))).item()\n",
    "    #     eff = 2 ** entropy_bits\n",
    "    #     print(f\"stage0 used={used0}/512 | entropy={entropy_bits:.2f} bits | eff_codes~{eff:.1f}\")\n",
    "    for s in range(rvq.R):\n",
    "        cs = counts[s]\n",
    "        p = cs / cs.sum()\n",
    "        p = p[p>0]\n",
    "        ent_bits = (-(p*p.log()).sum() / torch.log(torch.tensor(2.0, device=device))).item()\n",
    "        eff = 2**ent_bits\n",
    "        used = (cs>0).sum().item()\n",
    "        print(s, used, ent_bits, eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e1a9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def kmeans(latents, K, iters=25, batch_size=8192):\n",
    "    \"\"\"\n",
    "    latents: [N, D] (torch tensor, normalized if desired)\n",
    "    returns:\n",
    "      centroids: [K, D]\n",
    "      assignments: [N]\n",
    "    \"\"\"\n",
    "    device = latents.device\n",
    "    N, D = latents.shape\n",
    "\n",
    "    # --- init: random samples\n",
    "    perm = torch.randperm(N, device=device)\n",
    "    centroids = latents[perm[:K]].clone()  # [K, D]\n",
    "\n",
    "    for _ in range(iters):\n",
    "        counts = torch.zeros(K, device=device)\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            x = latents[i:i+batch_size]  # [B, D]\n",
    "\n",
    "            # squared L2 distances\n",
    "            x2 = (x ** 2).sum(dim=1, keepdim=True)\n",
    "            c2 = (centroids ** 2).sum(dim=1).unsqueeze(0)\n",
    "            dist = x2 + c2 - 2 * x @ centroids.t()\n",
    "\n",
    "            labels = dist.argmin(dim=1)  # [B]\n",
    "\n",
    "            for k in range(K):\n",
    "                mask = labels == k\n",
    "                if mask.any():\n",
    "                    new_centroids[k] += x[mask].sum(dim=0)\n",
    "                    counts[k] += mask.sum()\n",
    "\n",
    "        # avoid empty clusters\n",
    "        mask = counts > 0\n",
    "        centroids[mask] = new_centroids[mask] / counts[mask].unsqueeze(1)\n",
    "\n",
    "    # final assignment\n",
    "    assignments = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        x = latents[i:i+batch_size]\n",
    "        dist = (\n",
    "            (x**2).sum(1, keepdim=True)\n",
    "            + (centroids**2).sum(1)\n",
    "            - 2 * x @ centroids.t()\n",
    "        )\n",
    "        assignments.append(dist.argmin(dim=1))\n",
    "\n",
    "    return centroids, torch.cat(assignments)\n",
    "\n",
    "@torch.no_grad()\n",
    "def kmeans_recon_mse(latents, centroids, assignments):\n",
    "    recon = centroids[assignments]  # [N, D]\n",
    "    mse = ((latents - recon) ** 2).mean().item()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80655e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans K=2048 | mean_sq_L2=9207.786133 | per_dim_MSE=7.99287033\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "K = 2048   # IMPORTANT: try several values\n",
    "centroids, labels = kmeans(latents, K=K, iters=30)\n",
    "\n",
    "@torch.no_grad()\n",
    "def kmeans_recon_errors(latents, centroids, labels):\n",
    "    recon = centroids[labels]  # [N, D]\n",
    "\n",
    "    # mean squared L2 error: E ||z - z_hat||^2\n",
    "    mean_l2_sq = ((latents - recon) ** 2).sum(dim=1).mean().item()\n",
    "\n",
    "    # per-dimension MSE\n",
    "    mse = ((latents - recon) ** 2).mean().item()\n",
    "\n",
    "    return mean_l2_sq, mse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99342c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans K=2048 | mean_sq_L2=0.107094 | per_dim_MSE=0.00009296\n"
     ]
    }
   ],
   "source": [
    "mean_l2_sq, mse = kmeans_recon_errors(latents, centroids, labels)\n",
    "\n",
    "print(\n",
    "    f\"KMeans K={K} | mean_sq_L2={mean_l2_sq:.6f} | per_dim_MSE={mse:.8f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57db7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
