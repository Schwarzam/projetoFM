{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3656c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23473e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr6_splus = pd.read_csv(\"dr6_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc18c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HYDRA-0011</td>\n",
       "      <td>150.770833</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HYDRA-0012</td>\n",
       "      <td>152.308333</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HYDRA-0013</td>\n",
       "      <td>153.845833</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HYDRA-0014</td>\n",
       "      <td>155.383333</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HYDRA-0015</td>\n",
       "      <td>156.925000</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>STRIPE82-0166</td>\n",
       "      <td>355.750000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>STRIPE82-0167</td>\n",
       "      <td>357.166667</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>STRIPE82-0168</td>\n",
       "      <td>357.166667</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>STRIPE82-0169</td>\n",
       "      <td>358.583333</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>STRIPE82-0170</td>\n",
       "      <td>358.583333</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3050 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              field          ra        dec\n",
       "0        HYDRA-0011  150.770833 -23.908611\n",
       "1        HYDRA-0012  152.308333 -23.908611\n",
       "2        HYDRA-0013  153.845833 -23.908611\n",
       "3        HYDRA-0014  155.383333 -23.908611\n",
       "4        HYDRA-0015  156.925000 -23.908611\n",
       "...             ...         ...        ...\n",
       "3045  STRIPE82-0166  355.750000   0.700000\n",
       "3046  STRIPE82-0167  357.166667  -0.700000\n",
       "3047  STRIPE82-0168  357.166667   0.700000\n",
       "3048  STRIPE82-0169  358.583333  -0.700000\n",
       "3049  STRIPE82-0170  358.583333   0.700000\n",
       "\n",
       "[3050 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr6_splus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca9203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HYDRA-0011</td>\n",
       "      <td>150.770833</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HYDRA-0012</td>\n",
       "      <td>152.308333</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HYDRA-0013</td>\n",
       "      <td>153.845833</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HYDRA-0014</td>\n",
       "      <td>155.383333</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HYDRA-0015</td>\n",
       "      <td>156.925000</td>\n",
       "      <td>-23.908611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>STRIPE82-0166</td>\n",
       "      <td>355.750000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>STRIPE82-0167</td>\n",
       "      <td>357.166667</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>STRIPE82-0168</td>\n",
       "      <td>357.166667</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>STRIPE82-0169</td>\n",
       "      <td>358.583333</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>STRIPE82-0170</td>\n",
       "      <td>358.583333</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3050 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              field          ra        dec\n",
       "0        HYDRA-0011  150.770833 -23.908611\n",
       "1        HYDRA-0012  152.308333 -23.908611\n",
       "2        HYDRA-0013  153.845833 -23.908611\n",
       "3        HYDRA-0014  155.383333 -23.908611\n",
       "4        HYDRA-0015  156.925000 -23.908611\n",
       "...             ...         ...        ...\n",
       "3045  STRIPE82-0166  355.750000   0.700000\n",
       "3046  STRIPE82-0167  357.166667  -0.700000\n",
       "3047  STRIPE82-0168  357.166667   0.700000\n",
       "3048  STRIPE82-0169  358.583333  -0.700000\n",
       "3049  STRIPE82-0170  358.583333   0.700000\n",
       "\n",
       "[3050 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/home/astrodados/gaia_dr3/GaiaSource_463472-463488.csv.gz\n",
    "\n",
    "/home/astrodados3/splus/idr6/SPLUS-s02s01/SPLUS-s02s01_F378.fits.fz\n",
    "\n",
    "/home/astrodados4/downloads/desi_spectra/25861/20220124/coadd-0-25861-thru20220124.fits\n",
    "\n",
    "/home/astrodados3/dados/ztf22/dataset/Norder=2/Dir=0/Npix=137.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2d9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3050 rows from dr6_list.csv\n",
      "Indexed 3386 Gaia source chunks\n",
      "Indexed 3381 Gaia XP spectra chunks\n",
      "Loaded 6101 DESI tiles from desi_tiles.csv\n",
      "\n",
      "=== Field HYDRA-0011 (150.7708333, -23.90861111) ===\n",
      "  [DESI] Tiles within 1.7 deg of field center: 0\n",
      "  [SPLUS] Globbing S-PLUS files with pattern: /home/astrodados3/splus/idr6_final/main/HYDRA-0011*.fits\n",
      "  [SPLUS] Found 1 S-PLUS files for field HYDRA-0011.\n",
      "  [SPLUS] Opening S-PLUS file: /home/astrodados3/splus/idr6_final/main/HYDRA-0011_dual.fits\n",
      "  [SPLUS] Loaded 77127 rows for field HYDRA-0011\n",
      "  [SPLUS] Objects in this field file: 77127\n",
      "    [Field HYDRA-0011] chunk 0-77127\n",
      "      [Gaia] Processing chunk with 77127 S-PLUS rows\n",
      "      [Gaia] Files needed for this chunk: 2\n",
      "        [Gaia/ECSV] Opening /home/astrodados/gaia_dr3/GaiaSource_643214-644005.csv.gz\n",
      "        [Gaia/ECSV] Opening /home/astrodados/gaia_dr3/GaiaSource_644006-645213.csv.gz\n",
      "      [Gaia] Total Gaia rows loaded for this chunk: 1060042\n",
      "      [Gaia] Matched 19215 / 77127\n",
      "      [GaiaXP] Files needed: 2\n",
      "        [Gaia/ECSV] Opening /home/astrodados4/downloads/gaia_spectra/XpContinuousMeanSpectrum_643214-644005.csv.gz\n",
      "        [GaiaXP] 417 rows match in /home/astrodados4/downloads/gaia_spectra/XpContinuousMeanSpectrum_643214-644005.csv.gz\n",
      "        [Gaia/ECSV] Opening /home/astrodados4/downloads/gaia_spectra/XpContinuousMeanSpectrum_644006-645213.csv.gz\n",
      "        [GaiaXP] 190 rows match in /home/astrodados4/downloads/gaia_spectra/XpContinuousMeanSpectrum_644006-645213.csv.gz\n",
      "      [ZTF] Processing chunk with 77127 S-PLUS rows\n",
      "      [ZTF] Unique hpix with existing files: 0\n",
      "      [DESI] Processing chunk with 77127 S-PLUS rows\n",
      "      [DESI] Tiles considered for this field: 0\n",
      "  [OUT] Saved 77127 rows to datacube_HYDRA-0011.parquet\n",
      "\n",
      "=== Field HYDRA-0012 (152.3083333, -23.90861111) ===\n",
      "  [DESI] Tiles within 1.7 deg of field center: 0\n",
      "  [SPLUS] Globbing S-PLUS files with pattern: /home/astrodados3/splus/idr6_final/main/HYDRA-0012*.fits\n",
      "  [SPLUS] Found 1 S-PLUS files for field HYDRA-0012.\n",
      "  [SPLUS] Opening S-PLUS file: /home/astrodados3/splus/idr6_final/main/HYDRA-0012_dual.fits\n",
      "  [SPLUS] Loaded 80758 rows for field HYDRA-0012\n",
      "  [SPLUS] Objects in this field file: 80758\n",
      "    [Field HYDRA-0012] chunk 0-80758\n",
      "      [Gaia] Processing chunk with 80758 S-PLUS rows\n",
      "      [Gaia] Files needed for this chunk: 4\n",
      "        [Gaia/ECSV] Opening /home/astrodados/gaia_dr3/GaiaSource_621286-622263.csv.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 548\u001b[39m\n\u001b[32m    541\u001b[39m         build_datacube_for_field(row,\n\u001b[32m    542\u001b[39m                                  gaia_index,\n\u001b[32m    543\u001b[39m                                  gaia_spec_index,\n\u001b[32m    544\u001b[39m                                  desi_tiles,\n\u001b[32m    545\u001b[39m                                  desi_tiles_coord)\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 541\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Process ONE FIELD AT A TIME\u001b[39;00m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m dr6_splus.iterrows():\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[43mbuild_datacube_for_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mgaia_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mgaia_spec_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdesi_tiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdesi_tiles_coord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 486\u001b[39m, in \u001b[36mbuild_datacube_for_field\u001b[39m\u001b[34m(field_row, gaia_index, gaia_spec_index, desi_tiles, desi_tiles_coord)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    [Field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    484\u001b[39m chunk = splus_field.iloc[start:end].copy()\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m gaia_df   = \u001b[43mprocess_chunk_gaia\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaia_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m gaiaxp_df = process_chunk_gaia_spectra(gaia_df, gaia_spec_index)\n\u001b[32m    488\u001b[39m ztf_df    = process_chunk_ztf(chunk)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 232\u001b[39m, in \u001b[36mprocess_chunk_gaia\u001b[39m\u001b[34m(splus_chunk, gaia_index)\u001b[39m\n\u001b[32m    230\u001b[39m gaia_rows = []\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m needed_files:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     df = \u001b[43mread_gaia_ecsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     gaia_rows.append(df)\n\u001b[32m    235\u001b[39m gaia_cat = pd.concat(gaia_rows, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mread_gaia_ecsv\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[33;03mRead Gaia DR3 bulk-download file (GaiaSource_* or XpContinuousMeanSpectrum_*)\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03musing the required ECSV reader configuration.\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m        [Gaia/ECSV] Opening \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m tab = \u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mascii.ecsv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguess\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnull\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m99\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnan\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m99\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tab.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/astropy/table/connect.py:62\u001b[39m, in \u001b[36mTableRead.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m units = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33munits\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     60\u001b[39m descriptions = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdescriptions\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# For some readers (e.g., ascii.ecsv), the returned `out` class is not\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# guaranteed to be the same as the desired output `cls`.  If so,\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# try coercing to desired class without copying (io.registry.read\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# would normally do a copy).  The normal case here is swapping\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Table <=> QTable.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out.\u001b[34m__class__\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/astropy/io/registry/core.py:221\u001b[39m, in \u001b[36mUnifiedInputRegistry.read\u001b[39m\u001b[34m(self, cls, format, cache, *args, **kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m         kwargs.update({\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m: path})\n\u001b[32m    220\u001b[39m reader = \u001b[38;5;28mself\u001b[39m.get_reader(\u001b[38;5;28mformat\u001b[39m, \u001b[38;5;28mcls\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m data = \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mcls\u001b[39m):\n\u001b[32m    224\u001b[39m     \u001b[38;5;66;03m# User has read with a subclass where only the parent class is\u001b[39;00m\n\u001b[32m    225\u001b[39m     \u001b[38;5;66;03m# registered.  This returns the parent class, so try coercing\u001b[39;00m\n\u001b[32m    226\u001b[39m     \u001b[38;5;66;03m# to desired subclass.\u001b[39;00m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/astropy/io/ascii/connect.py:19\u001b[39m, in \u001b[36mio_read\u001b[39m\u001b[34m(format, filename, **kwargs)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mformat\u001b[39m = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^ascii\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[32m     18\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mformat\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/astropy/io/ascii/ui.py:488\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(table, guess, **kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    487\u001b[39m         reader = get_reader(**new_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         dat = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m         _read_trace.append(\n\u001b[32m    490\u001b[39m             {\n\u001b[32m    491\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m: copy.deepcopy(new_kwargs),\n\u001b[32m   (...)\u001b[39m\u001b[32m    494\u001b[39m             }\n\u001b[32m    495\u001b[39m         )\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Static analysis (pyright) indicates `dat` might be left undefined, so just\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# to be sure define it at the beginning and check here.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/astropy/io/ascii/core.py:1429\u001b[39m, in \u001b[36mBaseReader.read\u001b[39m\u001b[34m(self, table)\u001b[39m\n\u001b[32m   1426\u001b[39m     newline = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1428\u001b[39m \u001b[38;5;66;03m# Get a list of the lines (rows) in the table\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1429\u001b[39m \u001b[38;5;28mself\u001b[39m.lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minputter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;66;03m# Set self.data.data_lines to a slice of lines contain the data rows\u001b[39;00m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28mself\u001b[39m.data.get_data_lines(\u001b[38;5;28mself\u001b[39m.lines)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/astropy/io/ascii/core.py:340\u001b[39m, in \u001b[36mBaseInputter.get_lines\u001b[39m\u001b[34m(self, table, newline)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(table, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    336\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(table, os.PathLike)\n\u001b[32m    337\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m table + \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m table + \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    338\u001b[39m ):\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_readable_fileobj(table, encoding=\u001b[38;5;28mself\u001b[39m.encoding) \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m         table = \u001b[43mfileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     lines = table.splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/gzip.py:340\u001b[39m, in \u001b[36mGzipFile.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merrno\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno.EBADF, \u001b[33m\"\u001b[39m\u001b[33mread() on write-only GzipFile object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/_compression.py:118\u001b[39m, in \u001b[36mDecompressReader.readall\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m chunks = []\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# sys.maxsize means the max length of output buffer is unlimited,\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# so that the whole input buffer can be decompressed within one\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# .decompress() call.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m data := \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaxsize\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    119\u001b[39m     chunks.append(data)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/gzip.py:553\u001b[39m, in \u001b[36m_GzipReader.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.needs_input:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAD_BUFFER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m     uncompress = \u001b[38;5;28mself\u001b[39m._decompressor.decompress(buf, size)\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/gzip.py:96\u001b[39m, in \u001b[36m_PaddedFile.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read + size <= \u001b[38;5;28mself\u001b[39m._length:\n\u001b[32m     98\u001b[39m         read = \u001b[38;5;28mself\u001b[39m._read\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG – adjust paths/parameters to your setup\n",
    "# -------------------------------------------------\n",
    "\n",
    "SPLUS_BANDS = [\"R\", \"I\", \"G\", \"U\", \"Z\", \"F378\", \"F395\", \"F410\",\n",
    "               \"F430\", \"F515\", \"F660\", \"F861\"]\n",
    "\n",
    "SPLUS_IMAGES_PATH = [\"/home/astrodados3/splus/idr6/\"]\n",
    "\n",
    "# Your driver list (one field per line)\n",
    "DR6_LIST = \"dr6_list.csv\"   # columns: field, ra, dec  (ra/dec lowercase)\n",
    "\n",
    "# Pattern for S-PLUS dual catalogs (if ever needed)\n",
    "SPLUS_CAT_PATTERN = \"/home/astrodados3/splus/idr6_final/main/SPLUS-*_dual.fits\"\n",
    "\n",
    "# Gaia DR3 source chunks (GaiaSource_<start>-<end>.csv.gz, HEALPix level-8 ranges)\n",
    "GAIA_DIR = \"/home/astrodados/gaia_dr3\"\n",
    "\n",
    "# Gaia XP spectra chunks (XpContinuousMeanSpectrum_<start>-<end>.csv.gz, same scheme)\n",
    "GAIA_SPEC_DIR = \"/home/astrodados4/downloads/gaia_spectra\"\n",
    "\n",
    "# ZTF HiPS parquet\n",
    "ZTF_ROOT = \"/home/astrodados3/dados/ztf22/dataset\"\n",
    "\n",
    "# DESI spectra\n",
    "DESI_ROOT = \"/home/astrodados4/downloads/desi_spectra\"\n",
    "DESI_TILES_CSV = \"desi_tiles.csv\"   # must contain TILEID, TILERA, TILEDEC\n",
    "DESI_TILE_RADIUS_DEG = 1.7         # match S-PLUS field center to DESI tile center\n",
    "\n",
    "# HEALPix schemes\n",
    "# Gaia DR3 bulk download: files named GaiaSource_<start>-<end>.csv.gz\n",
    "# are contiguous ranges of HEALPix level-8 indices → NSIDE=256\n",
    "GAIA_NSIDE = 256\n",
    "GAIA_NEST = True\n",
    "\n",
    "GAIA_SPEC_NSIDE = GAIA_NSIDE\n",
    "GAIA_SPEC_NEST = GAIA_NEST\n",
    "\n",
    "# ZTF HiPS: Norder=2 → NSIDE = 2^2 = 4, NESTED\n",
    "ZTF_NORDER = 2\n",
    "ZTF_NSIDE = 2 ** ZTF_NORDER\n",
    "ZTF_NEST = True\n",
    "\n",
    "# Matching radii\n",
    "MATCH_RADIUS_SPLUS_GAIA = 0.5 * u.arcsec\n",
    "MATCH_RADIUS_SPLUS_ZTF  = 1.0 * u.arcsec\n",
    "MATCH_RADIUS_SPLUS_DESI = 1.0 * u.arcsec\n",
    "\n",
    "# How many S-PLUS rows to process at once (per field)\n",
    "CHUNK_SIZE = 300000\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Small helpers\n",
    "# -------------------------------------------------\n",
    "\n",
    "def radec_to_hpix(ra_deg, dec_deg, nside, nest=True):\n",
    "    ra = np.asarray(ra_deg)\n",
    "    dec = np.asarray(dec_deg)\n",
    "    theta = np.radians(90.0 - dec)\n",
    "    phi = np.radians(ra)\n",
    "    return hp.ang2pix(nside, theta, phi, nest=nest)\n",
    "\n",
    "def build_range_index(directory, prefix, suffix):\n",
    "    \"\"\"\n",
    "    Build list of (start, end, path) for files like:\n",
    "      prefix<start>-<end>suffix\n",
    "\n",
    "    For Gaia DR3 bulk download, <start>,<end> are HEALPix level-8 indices.\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    pattern = re.escape(prefix) + r\"(\\d+)-(\\d+)\" + re.escape(suffix)\n",
    "    for path in glob.glob(os.path.join(directory, prefix + \"*\" + suffix)):\n",
    "        fname = os.path.basename(path)\n",
    "        m = re.search(pattern, fname)\n",
    "        if not m:\n",
    "            continue\n",
    "        start = int(m.group(1))\n",
    "        end   = int(m.group(2))\n",
    "        entries.append((start, end, path))\n",
    "    entries.sort(key=lambda x: x[0])\n",
    "    return entries\n",
    "\n",
    "def files_for_hpix_ranges(hpix_indices, range_index):\n",
    "    \"\"\"\n",
    "    Given a list of HEALPix indices and a list of (start,end,path) ranges,\n",
    "    return the subset of files whose [start,end] overlaps those indices.\n",
    "    \"\"\"\n",
    "    hpix_set = set(int(h) for h in np.unique(hpix_indices))\n",
    "    files = set()\n",
    "    for start, end, path in range_index:\n",
    "        for h in list(hpix_set):\n",
    "            if start <= h <= end:\n",
    "                files.add(path)\n",
    "    return sorted(files)\n",
    "\n",
    "def ztf_file_for_hpix(hpix, norder=ZTF_NORDER, root=ZTF_ROOT):\n",
    "    hpix = int(hpix)\n",
    "    directory = hpix // 1000\n",
    "    return os.path.join(\n",
    "        root,\n",
    "        f\"Norder={norder}\",\n",
    "        f\"Dir={directory}\",\n",
    "        f\"Npix={hpix}.parquet\",\n",
    "    )\n",
    "\n",
    "def desi_file_paths_for_tiles(tile_ids):\n",
    "    \"\"\"\n",
    "    For a list of DESI TILEID values, return all coadd-*.fits files under\n",
    "    DESI_ROOT/<TILEID>/<NIGHT>/coadd-*-TILEID-thru*.fits (or fallback coadd-*.fits).\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for tileid in tile_ids:\n",
    "        tileid = int(tileid)\n",
    "        tile_dir = os.path.join(DESI_ROOT, str(tileid))\n",
    "        if not os.path.isdir(tile_dir):\n",
    "            print(f\"      [DESI] Tile dir not found for TILEID={tileid}: {tile_dir}\")\n",
    "            continue\n",
    "\n",
    "        pattern_main = os.path.join(tile_dir, \"*\", f\"coadd-*-{tileid}-thru*.fits\")\n",
    "        tile_paths = glob.glob(pattern_main)\n",
    "        if not tile_paths:\n",
    "            # fallback: any coadd file\n",
    "            pattern_fallback = os.path.join(tile_dir, \"*\", \"coadd-*.fits\")\n",
    "            tile_paths = glob.glob(pattern_fallback)\n",
    "\n",
    "        tile_paths = sorted(tile_paths)\n",
    "        if not tile_paths:\n",
    "            print(f\"      [DESI] No coadd files in {tile_dir}\")\n",
    "        else:\n",
    "            print(f\"      [DESI] TILEID {tileid} → {len(tile_paths)} coadd files\")\n",
    "            paths.extend(tile_paths)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def match_catalogs(src_coord, cat_coord, max_sep):\n",
    "    idx, d2d, _ = src_coord.match_to_catalog_sky(cat_coord)\n",
    "    idx[d2d > max_sep] = -1\n",
    "    return idx\n",
    "\n",
    "def read_gaia_ecsv(path):\n",
    "    \"\"\"\n",
    "    Read Gaia DR3 bulk-download file (GaiaSource_* or XpContinuousMeanSpectrum_*)\n",
    "    using the required ECSV reader configuration.\n",
    "    \"\"\"\n",
    "    print(f\"        [Gaia/ECSV] Opening {path}\")\n",
    "    tab = Table.read(\n",
    "        path,\n",
    "        format=\"ascii.ecsv\",\n",
    "        guess=False,\n",
    "        fill_values=[('null', 99), ('nan', 99)],\n",
    "    )\n",
    "    return tab.to_pandas()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# S-PLUS per-field\n",
    "# -------------------------------------------------\n",
    "\n",
    "def get_splus_path_for_field(field_name):\n",
    "    \"\"\"\n",
    "    Return the S-PLUS dual catalog FITS path for a given field.\n",
    "\n",
    "    Here we assume files like:\n",
    "      /home/astrodados3/splus/idr6_final/main/HYDRA-0011_dual.fits\n",
    "    i.e., <FIELD>_dual.fits\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(\n",
    "        \"/home/astrodados3/splus/idr6_final/main\",\n",
    "        f\"{field_name}*.fits\"\n",
    "    )\n",
    "    print(f\"  [SPLUS] Globbing S-PLUS files with pattern: {pattern}\")\n",
    "    candidates = glob.glob(pattern)\n",
    "    print(f\"  [SPLUS] Found {len(candidates)} S-PLUS files for field {field_name}.\")\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Please define S-PLUS file path for field '{field_name}' \"\n",
    "        f\"in get_splus_path_for_field().\"\n",
    "    )\n",
    "\n",
    "def load_splus_for_field(field_row):\n",
    "    \"\"\"\n",
    "    Open ONLY the S-PLUS catalog corresponding to this field.\n",
    "    Assumes entire tile is in one FITS file.\n",
    "    \"\"\"\n",
    "    field_name = field_row[\"field\"]\n",
    "    fits_path = get_splus_path_for_field(field_name)\n",
    "    print(f\"  [SPLUS] Opening S-PLUS file: {fits_path}\")\n",
    "    tab = Table.read(fits_path)\n",
    "    df = tab.to_pandas()\n",
    "    if \"ra\" not in df.columns or \"dec\" not in df.columns:\n",
    "        raise ValueError(f\"{fits_path} has no 'ra'/'dec' columns.\")\n",
    "    print(f\"  [SPLUS] Loaded {len(df)} rows for field {field_name}\")\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Per-chunk external survey processing\n",
    "# -------------------------------------------------\n",
    "\n",
    "def process_chunk_gaia(splus_chunk, gaia_index):\n",
    "    if splus_chunk.empty:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [Gaia] Processing chunk with {len(splus_chunk)} S-PLUS rows\")\n",
    "\n",
    "    splus_coord = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    hpix_gaia = radec_to_hpix(\n",
    "        splus_chunk[\"ra\"], splus_chunk[\"dec\"], GAIA_NSIDE, GAIA_NEST\n",
    "    )\n",
    "\n",
    "    needed_files = files_for_hpix_ranges(hpix_gaia, gaia_index)\n",
    "    print(f\"      [Gaia] Files needed for this chunk: {len(needed_files)}\")\n",
    "    if not needed_files:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    gaia_rows = []\n",
    "    for path in needed_files:\n",
    "        df = read_gaia_ecsv(path)\n",
    "        gaia_rows.append(df)\n",
    "\n",
    "    gaia_cat = pd.concat(gaia_rows, ignore_index=True)\n",
    "    print(f\"      [Gaia] Total Gaia rows loaded for this chunk: {len(gaia_cat)}\")\n",
    "\n",
    "    if \"ra\" not in gaia_cat.columns or \"dec\" not in gaia_cat.columns:\n",
    "        raise ValueError(\"Gaia catalog has no 'ra'/'dec' columns.\")\n",
    "\n",
    "    gaia_coord = SkyCoord(\n",
    "        gaia_cat[\"ra\"].values * u.deg,\n",
    "        gaia_cat[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    match_idx = match_catalogs(splus_coord, gaia_coord, MATCH_RADIUS_SPLUS_GAIA)\n",
    "    n_matches = np.sum(match_idx != -1)\n",
    "    print(f\"      [Gaia] Matched {n_matches} / {len(splus_chunk)}\")\n",
    "\n",
    "    out = []\n",
    "    for idx in match_idx:\n",
    "        out.append({} if idx == -1 else gaia_cat.iloc[int(idx)].to_dict())\n",
    "\n",
    "    gaia_df = pd.DataFrame(out, index=splus_chunk.index)\n",
    "    if not gaia_df.empty:\n",
    "        gaia_df.columns = [f\"gaia_{c}\" for c in gaia_df.columns]\n",
    "\n",
    "    return gaia_df\n",
    "\n",
    "def process_chunk_gaia_spectra(gaia_df, gaia_spec_index):\n",
    "    if gaia_df.empty or \"gaia_source_id\" not in gaia_df.columns:\n",
    "        print(\"      [GaiaXP] No Gaia matches or missing 'gaia_source_id'.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    sid_series = gaia_df[\"gaia_source_id\"].dropna()\n",
    "    if sid_series.empty:\n",
    "        print(\"      [GaiaXP] No valid source_ids.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    if not {\"gaia_ra\", \"gaia_dec\"}.issubset(gaia_df.columns):\n",
    "        print(\"      [GaiaXP] Missing 'gaia_ra'/'gaia_dec'.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    ra = gaia_df[\"gaia_ra\"].astype(float)\n",
    "    dec = gaia_df[\"gaia_dec\"].astype(float)\n",
    "    mask_valid = (~ra.isna()) & (~dec.isna()) & (~gaia_df[\"gaia_source_id\"].isna())\n",
    "    if not mask_valid.any():\n",
    "        print(\"      [GaiaXP] No rows with valid RA/DEC + source_id.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    hpix_spec = radec_to_hpix(\n",
    "        ra[mask_valid].values,\n",
    "        dec[mask_valid].values,\n",
    "        GAIA_SPEC_NSIDE,\n",
    "        GAIA_SPEC_NEST,\n",
    "    )\n",
    "    needed_files = files_for_hpix_ranges(hpix_spec, gaia_spec_index)\n",
    "    print(f\"      [GaiaXP] Files needed: {len(needed_files)}\")\n",
    "\n",
    "    if not needed_files:\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    needed_source_ids = set(sid_series.astype(\"int64\").values.tolist())\n",
    "\n",
    "    spec_rows = []\n",
    "    for path in needed_files:\n",
    "        df = read_gaia_ecsv(path)\n",
    "        if \"source_id\" not in df.columns:\n",
    "            print(f\"        [GaiaXP] 'source_id' missing in {path}, skipping.\")\n",
    "            continue\n",
    "        df = df[df[\"source_id\"].isin(needed_source_ids)]\n",
    "        print(f\"        [GaiaXP] {len(df)} rows match in {path}\")\n",
    "        if not df.empty:\n",
    "            spec_rows.append(df)\n",
    "\n",
    "    if not spec_rows:\n",
    "        print(\"      [GaiaXP] No XP spectra matched.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    spec_cat = pd.concat(spec_rows, ignore_index=True)\n",
    "    spec_unique = spec_cat.drop_duplicates(subset=\"source_id\", keep=\"first\")\n",
    "    spec_unique = spec_unique.set_index(\"source_id\")\n",
    "\n",
    "    out = []\n",
    "    for sid in gaia_df[\"gaia_source_id\"]:\n",
    "        if pd.isna(sid):\n",
    "            out.append({})\n",
    "        else:\n",
    "            sid_int = int(sid)\n",
    "            if sid_int in spec_unique.index:\n",
    "                out.append(spec_unique.loc[sid_int].to_dict())\n",
    "            else:\n",
    "                out.append({})\n",
    "\n",
    "    gaiaxp_df = pd.DataFrame(out, index=gaia_df.index)\n",
    "    if not gaiaxp_df.empty:\n",
    "        gaiaxp_df.columns = [f\"gaiaxp_{c}\" for c in gaiaxp_df.columns]\n",
    "\n",
    "    return gaiaxp_df\n",
    "\n",
    "def process_chunk_ztf(splus_chunk):\n",
    "    if splus_chunk.empty:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [ZTF] Processing chunk with {len(splus_chunk)} S-PLUS rows\")\n",
    "\n",
    "    splus_coord = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    hpix_ztf = radec_to_hpix(\n",
    "        splus_chunk[\"ra\"], splus_chunk[\"dec\"], ZTF_NSIDE, ZTF_NEST\n",
    "    )\n",
    "\n",
    "    files_by_hpix = defaultdict(list)\n",
    "    for hpix in np.unique(hpix_ztf):\n",
    "        path = ztf_file_for_hpix(hpix)\n",
    "        if os.path.exists(path):\n",
    "            files_by_hpix[int(hpix)].append(path)\n",
    "\n",
    "    print(f\"      [ZTF] Unique hpix with existing files: {len(files_by_hpix)}\")\n",
    "\n",
    "    if not files_by_hpix:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    ztf_rows = []\n",
    "    for hpix, paths in files_by_hpix.items():\n",
    "        print(f\"        [ZTF] hpix={hpix} → {len(paths)} parquet files\")\n",
    "        for path in paths:\n",
    "            tab = pq.read_table(path).to_pandas()\n",
    "            ztf_rows.append(tab)\n",
    "\n",
    "    if not ztf_rows:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    ztf_cat = pd.concat(ztf_rows, ignore_index=True)\n",
    "    print(f\"      [ZTF] Total ZTF rows loaded: {len(ztf_cat)}\")\n",
    "\n",
    "    if \"ra\" not in ztf_cat.columns or \"dec\" not in ztf_cat.columns:\n",
    "        raise ValueError(\"ZTF parquet must contain 'ra'/'dec' columns.\")\n",
    "\n",
    "    ztf_coord = SkyCoord(\n",
    "        ztf_cat[\"ra\"].values * u.deg,\n",
    "        ztf_cat[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    match_idx = match_catalogs(splus_coord, ztf_coord, MATCH_RADIUS_SPLUS_ZTF)\n",
    "    n_matches = np.sum(match_idx != -1)\n",
    "    print(f\"      [ZTF] Matched {n_matches} / {len(splus_chunk)}\")\n",
    "\n",
    "    out = []\n",
    "    for idx in match_idx:\n",
    "        if idx == -1:\n",
    "            out.append({})\n",
    "        else:\n",
    "            out.append(ztf_cat.iloc[int(idx)].to_dict())\n",
    "\n",
    "    ztf_df = pd.DataFrame(out, index=splus_chunk.index)\n",
    "    if not ztf_df.empty:\n",
    "        ztf_df.columns = [f\"ztf_{c}\" for c in ztf_df.columns]\n",
    "\n",
    "    return ztf_df\n",
    "\n",
    "def process_chunk_desi(splus_chunk, desi_tile_ids):\n",
    "    if splus_chunk.empty:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [DESI] Processing chunk with {len(splus_chunk)} S-PLUS rows\")\n",
    "    print(f\"      [DESI] Tiles considered for this field: {len(desi_tile_ids)}\")\n",
    "\n",
    "    if not desi_tile_ids:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    splus_coord = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "\n",
    "    desi_rows = [ {} for _ in range(len(splus_chunk)) ]\n",
    "\n",
    "    paths = desi_file_paths_for_tiles(desi_tile_ids)\n",
    "    print(f\"      [DESI] Total DESI coadd files to inspect: {len(paths)}\")\n",
    "\n",
    "    for path in paths:\n",
    "        print(f\"        [DESI] Opening {path}\")\n",
    "        with fits.open(path) as hd:\n",
    "            if \"FIBERMAP\" not in hd:\n",
    "                print(\"        [DESI] No FIBERMAP extension, skipping.\")\n",
    "                continue\n",
    "            fibermap = Table(hd[\"FIBERMAP\"].data).to_pandas()\n",
    "\n",
    "        # Drop fibermap rows with NaN coordinates\n",
    "        if \"TARGET_RA\" not in fibermap.columns or \"TARGET_DEC\" not in fibermap.columns:\n",
    "            print(\"        [DESI] FIBERMAP missing TARGET_RA/DEC, skipping.\")\n",
    "            continue\n",
    "\n",
    "        fibermap = fibermap.dropna(subset=[\"TARGET_RA\", \"TARGET_DEC\"])\n",
    "        if fibermap.empty:\n",
    "            print(\"        [DESI] All TARGET_RA/DEC are NaN in this file, skipping.\")\n",
    "            continue\n",
    "\n",
    "        desi_coord = SkyCoord(\n",
    "            fibermap[\"TARGET_RA\"].values * u.deg,\n",
    "            fibermap[\"TARGET_DEC\"].values * u.deg,\n",
    "        )\n",
    "        match_idx = match_catalogs(splus_coord, desi_coord, MATCH_RADIUS_SPLUS_DESI)\n",
    "        n_matches = np.sum(match_idx != -1)\n",
    "        print(f\"        [DESI] Matched {n_matches} / {len(splus_chunk)} in this file\")\n",
    "\n",
    "        for i, idx in enumerate(match_idx):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            if not desi_rows[i]:\n",
    "                row = dict(fibermap.iloc[int(idx)])\n",
    "                row[\"desi_file\"] = path\n",
    "                desi_rows[i] = row\n",
    "\n",
    "    desi_df = pd.DataFrame(desi_rows, index=splus_chunk.index)\n",
    "    if not desi_df.empty:\n",
    "        desi_df.columns = [f\"desi_{c}\" for c in desi_df.columns]\n",
    "\n",
    "    return desi_df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Per-field datacube\n",
    "# -------------------------------------------------\n",
    "\n",
    "def build_datacube_for_field(field_row,\n",
    "                             gaia_index,\n",
    "                             gaia_spec_index,\n",
    "                             desi_tiles,\n",
    "                             desi_tiles_coord):\n",
    "    field_name = field_row[\"field\"]\n",
    "    field_ra = float(field_row[\"ra\"])\n",
    "    field_dec = float(field_row[\"dec\"])\n",
    "\n",
    "    print(f\"\\n=== Field {field_name} ({field_ra}, {field_dec}) ===\")\n",
    "\n",
    "    # 0. Determine DESI tiles near this field (based on field center)\n",
    "    field_center = SkyCoord(field_ra * u.deg, field_dec * u.deg)\n",
    "    sep = field_center.separation(desi_tiles_coord)\n",
    "    mask_tiles = sep.deg < DESI_TILE_RADIUS_DEG\n",
    "    field_tile_ids = desi_tiles.loc[mask_tiles, \"TILEID\"].astype(int).tolist()\n",
    "    print(f\"  [DESI] Tiles within {DESI_TILE_RADIUS_DEG} deg of field center: {len(field_tile_ids)}\")\n",
    "\n",
    "    # 1. Open ONLY the S-PLUS catalog for this field\n",
    "    splus_field = load_splus_for_field(field_row)\n",
    "    print(f\"  [SPLUS] Objects in this field file: {len(splus_field)}\")\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    # 2. Process this field in chunks (for memory)\n",
    "    for start in range(0, len(splus_field), CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, len(splus_field))\n",
    "        print(f\"    [Field {field_name}] chunk {start}-{end}\")\n",
    "        chunk = splus_field.iloc[start:end].copy()\n",
    "\n",
    "        gaia_df   = process_chunk_gaia(chunk, gaia_index)\n",
    "        gaiaxp_df = process_chunk_gaia_spectra(gaia_df, gaia_spec_index)\n",
    "        ztf_df    = process_chunk_ztf(chunk)\n",
    "        desi_df   = process_chunk_desi(chunk, field_tile_ids)\n",
    "\n",
    "        merged = (chunk\n",
    "                  .join(gaia_df)\n",
    "                  .join(gaiaxp_df)\n",
    "                  .join(ztf_df)\n",
    "                  .join(desi_df))\n",
    "        all_chunks.append(merged)\n",
    "\n",
    "    datacube_field = pd.concat(all_chunks, ignore_index=True)\n",
    "    out_name = f\"datacube_{field_name}.parquet\"\n",
    "    datacube_field.to_parquet(out_name)\n",
    "    print(f\"  [OUT] Saved {len(datacube_field)} rows to {out_name}\")\n",
    "\n",
    "    # free memory for this field\n",
    "    del datacube_field\n",
    "    del all_chunks\n",
    "    del splus_field\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main\n",
    "# -------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # Driver list\n",
    "    dr6_splus = pd.read_csv(DR6_LIST)\n",
    "    print(f\"Loaded {len(dr6_splus)} rows from {DR6_LIST}\")\n",
    "\n",
    "    # Gaia indices (HEALPix level-8 ranges, small metadata kept once)\n",
    "    gaia_index = build_range_index(GAIA_DIR, \"GaiaSource_\", \".csv.gz\")\n",
    "    print(f\"Indexed {len(gaia_index)} Gaia source chunks\")\n",
    "\n",
    "    gaia_spec_index = build_range_index(\n",
    "        GAIA_SPEC_DIR, \"XpContinuousMeanSpectrum_\", \".csv.gz\"\n",
    "    )\n",
    "    print(f\"Indexed {len(gaia_spec_index)} Gaia XP spectra chunks\")\n",
    "\n",
    "    # DESI tiles\n",
    "    desi_tiles = pd.read_csv(DESI_TILES_CSV)\n",
    "    # Expect TILEID, TILERA, TILEDEC\n",
    "    for col in (\"TILEID\", \"TILERA\", \"TILEDEC\"):\n",
    "        if col not in desi_tiles.columns:\n",
    "            raise ValueError(f\"{DESI_TILES_CSV} is missing '{col}' column.\")\n",
    "    print(f\"Loaded {len(desi_tiles)} DESI tiles from {DESI_TILES_CSV}\")\n",
    "\n",
    "    desi_tiles_coord = SkyCoord(\n",
    "        desi_tiles[\"TILERA\"].values * u.deg,\n",
    "        desi_tiles[\"TILEDEC\"].values * u.deg,\n",
    "    )\n",
    "\n",
    "    # Process ONE FIELD AT A TIME\n",
    "    for _, row in dr6_splus.iterrows():\n",
    "        build_datacube_for_field(row,\n",
    "                                 gaia_index,\n",
    "                                 gaia_spec_index,\n",
    "                                 desi_tiles,\n",
    "                                 desi_tiles_coord)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e4ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 rows from dr6_list.csv\n",
      "Indexed 3386 Gaia source chunks\n",
      "Indexed 3381 Gaia XP spectra chunks\n",
      "Loaded 6101 DESI tiles from desi_tiles.csv\n",
      "Loading APOGEE catalog from /home/astrodados4/downloads/allStar-dr17-synspec_rev1.fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: hdu= was not specified but multiple tables are present, reading in first available table (hdu=1) [astropy.io.fits.connect]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 733900 APOGEE rows with valid RA/DEC and 1D columns\n",
      "\n",
      "=== Field STRIPE82-0001 (0.0, -0.7) ===\n",
      "  [DESI] Tiles within 1.7 deg of field center: 3\n",
      "  [SPLUS] Globbing S-PLUS files with pattern: /home/astrodados3/splus/idr6_final/main/STRIPE82-0001*.fits\n",
      "  [SPLUS] Found 1 S-PLUS files for field STRIPE82-0001.\n",
      "  [SPLUS] Opening S-PLUS file: /home/astrodados3/splus/idr6_final/main/STRIPE82-0001_dual.fits\n",
      "  [SPLUS] Loaded 53997 rows for field STRIPE82-0001\n",
      "  [SPLUS] Objects in this field file: 53997\n",
      "    [Field STRIPE82-0001] chunk 0-53997\n",
      "      [Gaia] Processing chunk with 53997 S-PLUS rows\n",
      "      [Gaia] Files needed for this chunk: 3\n",
      "        [Gaia/ECSV] Opening /home/astrodados/gaia_dr3/GaiaSource_277913-281529.csv.gz\n",
      "        [Gaia/ECSV] Opening /home/astrodados/gaia_dr3/GaiaSource_288627-292004.csv.gz\n",
      "        [Gaia/ECSV] Opening /home/astrodados/gaia_dr3/GaiaSource_299573-302248.csv.gz\n",
      "      [Gaia] Total Gaia rows loaded for this chunk: 1646076\n",
      "      [Gaia] Matched 5673 / 53997\n",
      "    [SPLUS IMG] Gaia-matched rows for cutouts: 5673 / 53997\n",
      "      [SPLUS IMG] Making cutouts for chunk with 53997 rows\n",
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_R.fits.fz for band R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-09-15T03:27:06.762' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_I.fits.fz for band I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-09-15T03:31:12.338' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_G.fits.fz for band G\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-09-15T03:23:18.911' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_U.fits.fz for band U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-10-07T03:40:32.683' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_Z.fits.fz for band Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-09-15T03:35:40.393' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_F378.fits.fz for band F378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-10-07T03:54:02.483' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_F395.fits.fz for band F395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-10-07T04:07:08.458' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_F410.fits.fz for band F410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-10-07T04:15:06.464' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_F430.fits.fz for band F430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-10-07T04:20:21.733' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_F515.fits.fz for band F515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-09-15T03:18:02.782' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_F660.fits.fz for band F660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-09-15T03:40:52.267' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SPLUS IMG] Using image /home/astrodados3/splus/idr6/STRIPE82-0001/STRIPE82-0001_F861.fits.fz for band F861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: FITSFixedWarning: 'datfix' made the change 'Set DATE-OBS to '2018-09-15T03:57:27.020' from MJD-OBS'. [astropy.wcs.wcs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [GaiaXP] Files needed: 3\n",
      "        [Gaia/ECSV] Opening /home/astrodados4/downloads/gaia_spectra/XpContinuousMeanSpectrum_277913-281529.csv.gz\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.wcs import WCS\n",
    "from astropy.nddata import Cutout2D\n",
    "import astropy.units as u\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG – adjust paths/parameters to your setup\n",
    "# -------------------------------------------------\n",
    "\n",
    "SPLUS_BANDS = [\"R\", \"I\", \"G\", \"U\", \"Z\", \"F378\", \"F395\", \"F410\",\n",
    "               \"F430\", \"F515\", \"F660\", \"F861\"]\n",
    "\n",
    "# Base dirs where images live; we’ll search in each\n",
    "SPLUS_IMAGES_PATH = [\"/home/astrodados3/splus/idr6/\"]\n",
    "\n",
    "# Cutout configuration\n",
    "SPLUS_CUTOUT_SIZE = 96  # pixels\n",
    "# If True, only make S-PLUS cutouts for objects that have a Gaia match\n",
    "ONLY_GAIA_MATCH_CUTOUTS = True\n",
    "\n",
    "# Your driver list (one field per line)\n",
    "DR6_LIST = \"dr6_list.csv\"   # columns: field, ra, dec  (ra/dec lowercase)\n",
    "\n",
    "# Pattern for S-PLUS dual catalogs (if ever needed)\n",
    "SPLUS_CAT_PATTERN = \"/home/astrodados3/splus/idr6_final/main/SPLUS-*_dual.fits\"\n",
    "\n",
    "# Gaia DR3 source chunks (GaiaSource_<start>-<end>.csv.gz, HEALPix level-8 ranges)\n",
    "GAIA_DIR = \"/home/astrodados/gaia_dr3\"\n",
    "\n",
    "# Gaia XP spectra chunks (XpContinuousMeanSpectrum_<start>-<end>.csv.gz, same scheme)\n",
    "GAIA_SPEC_DIR = \"/home/astrodados4/downloads/gaia_spectra\"\n",
    "\n",
    "# ZTF HiPS parquet\n",
    "ZTF_ROOT = \"/home/astrodados3/dados/ztf22/dataset\"\n",
    "\n",
    "# DESI spectra\n",
    "DESI_ROOT = \"/home/astrodados4/downloads/desi_spectra\"\n",
    "DESI_TILES_CSV = \"desi_tiles.csv\"   # must contain TILEID, TILERA, TILEDEC\n",
    "DESI_TILE_RADIUS_DEG = 1.7         # match S-PLUS field center to DESI tile center\n",
    "\n",
    "# APOGEE allStar DR17\n",
    "APOGEE_PATH = \"/home/astrodados4/downloads/allStar-dr17-synspec_rev1.fits\"\n",
    "\n",
    "# HEALPix schemes\n",
    "# Gaia DR3 bulk download: files named GaiaSource_<start>-<end>.csv.gz\n",
    "# are contiguous ranges of HEALPix level-8 indices → NSIDE=256\n",
    "GAIA_NSIDE = 256\n",
    "GAIA_NEST = True\n",
    "\n",
    "GAIA_SPEC_NSIDE = GAIA_NSIDE\n",
    "GAIA_SPEC_NEST = GAIA_NEST\n",
    "\n",
    "# ZTF HiPS: Norder=2 → NSIDE = 2^2 = 4, NESTED\n",
    "ZTF_NORDER = 2\n",
    "ZTF_NSIDE = 2 ** ZTF_NORDER\n",
    "ZTF_NEST = True\n",
    "\n",
    "# Matching radii\n",
    "MATCH_RADIUS_SPLUS_GAIA    = 0.5 * u.arcsec\n",
    "MATCH_RADIUS_SPLUS_ZTF     = 1.0 * u.arcsec\n",
    "MATCH_RADIUS_SPLUS_DESI    = 1.0 * u.arcsec\n",
    "MATCH_RADIUS_SPLUS_APOGEE  = 1.0 * u.arcsec\n",
    "\n",
    "# How many S-PLUS rows to process at once (per field)\n",
    "CHUNK_SIZE = 300000\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Small helpers\n",
    "# -------------------------------------------------\n",
    "\n",
    "def radec_to_hpix(ra_deg, dec_deg, nside, nest=True):\n",
    "    ra = np.asarray(ra_deg)\n",
    "    dec = np.asarray(dec_deg)\n",
    "    theta = np.radians(90.0 - dec)\n",
    "    phi = np.radians(ra)\n",
    "    return hp.ang2pix(nside, theta, phi, nest=nest)\n",
    "\n",
    "def build_range_index(directory, prefix, suffix):\n",
    "    \"\"\"\n",
    "    Build list of (start, end, path) for files like:\n",
    "      prefix<start>-<end>suffix\n",
    "\n",
    "    For Gaia DR3 bulk download, <start>,<end> are HEALPix level-8 indices.\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    pattern = re.escape(prefix) + r\"(\\d+)-(\\d+)\" + re.escape(suffix)\n",
    "    for path in glob.glob(os.path.join(directory, prefix + \"*\" + suffix)):\n",
    "        fname = os.path.basename(path)\n",
    "        m = re.search(pattern, fname)\n",
    "        if not m:\n",
    "            continue\n",
    "        start = int(m.group(1))\n",
    "        end   = int(m.group(2))\n",
    "        entries.append((start, end, path))\n",
    "    entries.sort(key=lambda x: x[0])\n",
    "    return entries\n",
    "\n",
    "def files_for_hpix_ranges(hpix_indices, range_index):\n",
    "    \"\"\"\n",
    "    Given a list of HEALPix indices and a list of (start,end,path) ranges,\n",
    "    return the subset of files whose [start,end] overlaps those indices.\n",
    "    \"\"\"\n",
    "    hpix_set = set(int(h) for h in np.unique(hpix_indices))\n",
    "    files = set()\n",
    "    for start, end, path in range_index:\n",
    "        for h in list(hpix_set):\n",
    "            if start <= h <= end:\n",
    "                files.add(path)\n",
    "    return sorted(files)\n",
    "\n",
    "def ztf_file_for_hpix(hpix, norder=ZTF_NORDER, root=ZTF_ROOT):\n",
    "    hpix = int(hpix)\n",
    "    directory = hpix // 1000\n",
    "    return os.path.join(\n",
    "        root,\n",
    "        f\"Norder={norder}\",\n",
    "        f\"Dir={directory}\",\n",
    "        f\"Npix={hpix}.parquet\",\n",
    "    )\n",
    "\n",
    "def desi_file_paths_for_tiles(tile_ids):\n",
    "    \"\"\"\n",
    "    For a list of DESI TILEID values, return all coadd-*.fits files under\n",
    "    DESI_ROOT/<TILEID>/<NIGHT>/coadd-*-TILEID-thru*.fits (or fallback coadd-*.fits).\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for tileid in tile_ids:\n",
    "        tileid = int(tileid)\n",
    "        tile_dir = os.path.join(DESI_ROOT, str(tileid))\n",
    "        if not os.path.isdir(tile_dir):\n",
    "            print(f\"      [DESI] Tile dir not found for TILEID={tileid}: {tile_dir}\")\n",
    "            continue\n",
    "\n",
    "        pattern_main = os.path.join(tile_dir, \"*\", f\"coadd-*-{tileid}-thru*.fits\")\n",
    "        tile_paths = glob.glob(pattern_main)\n",
    "        if not tile_paths:\n",
    "            # fallback: any coadd file\n",
    "            pattern_fallback = os.path.join(tile_dir, \"*\", \"coadd-*.fits\")\n",
    "            tile_paths = glob.glob(pattern_fallback)\n",
    "\n",
    "        tile_paths = sorted(tile_paths)\n",
    "        if not tile_paths:\n",
    "            print(f\"      [DESI] No coadd files in {tile_dir}\")\n",
    "        else:\n",
    "            print(f\"      [DESI] TILEID {tileid} → {len(tile_paths)} coadd files\")\n",
    "            paths.extend(tile_paths)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def match_catalogs(src_coord, cat_coord, max_sep):\n",
    "    idx, d2d, _ = src_coord.match_to_catalog_sky(cat_coord)\n",
    "    idx[d2d > max_sep] = -1\n",
    "    return idx\n",
    "\n",
    "def read_gaia_ecsv(path):\n",
    "    \"\"\"\n",
    "    Read Gaia DR3 bulk-download file (GaiaSource_* or XpContinuousMeanSpectrum_*)\n",
    "    using the required ECSV reader configuration.\n",
    "    \"\"\"\n",
    "    print(f\"        [Gaia/ECSV] Opening {path}\")\n",
    "    tab = Table.read(\n",
    "        path,\n",
    "        format=\"ascii.ecsv\",\n",
    "        guess=False,\n",
    "        fill_values=[('null', 99), ('nan', 99)],\n",
    "    )\n",
    "    return tab.to_pandas()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# S-PLUS per-field (catalog)\n",
    "# -------------------------------------------------\n",
    "\n",
    "def get_splus_path_for_field(field_name):\n",
    "    \"\"\"\n",
    "    Return the S-PLUS dual catalog FITS path for a given field.\n",
    "\n",
    "    Here we assume files like:\n",
    "      /home/astrodados3/splus/idr6_final/main/HYDRA-0011_dual.fits\n",
    "    i.e., <FIELD>_dual.fits\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(\n",
    "        \"/home/astrodados3/splus/idr6_final/main\",\n",
    "        f\"{field_name}*.fits\"\n",
    "    )\n",
    "    print(f\"  [SPLUS] Globbing S-PLUS files with pattern: {pattern}\")\n",
    "    candidates = glob.glob(pattern)\n",
    "    print(f\"  [SPLUS] Found {len(candidates)} S-PLUS files for field {field_name}.\")\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Please define S-PLUS file path for field '{field_name}' \"\n",
    "        f\"in get_splus_path_for_field().\"\n",
    "    )\n",
    "\n",
    "def load_splus_for_field(field_row):\n",
    "    \"\"\"\n",
    "    Open ONLY the S-PLUS catalog corresponding to this field.\n",
    "    Assumes entire tile is in one FITS file.\n",
    "    \"\"\"\n",
    "    field_name = field_row[\"field\"]\n",
    "    fits_path = get_splus_path_for_field(field_name)\n",
    "    print(f\"  [SPLUS] Opening S-PLUS file: {fits_path}\")\n",
    "    tab = Table.read(fits_path)\n",
    "    df = tab.to_pandas()\n",
    "    if \"ra\" not in df.columns or \"dec\" not in df.columns:\n",
    "        raise ValueError(f\"{fits_path} has no 'ra'/'dec' columns.\")\n",
    "    print(f\"  [SPLUS] Loaded {len(df)} rows for field {field_name}\")\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# S-PLUS image cutouts (stored in parquet)\n",
    "# -------------------------------------------------\n",
    "\n",
    "def get_splus_image_path_for_band(field_name, band):\n",
    "    \"\"\"\n",
    "    Find the S-PLUS image file for a given field and band,\n",
    "    trying all base paths in SPLUS_IMAGES_PATH.\n",
    "    Expected pattern:\n",
    "      <base>/<FIELD>/<FIELD>_<BAND>.fits.fz\n",
    "    Example:\n",
    "      /home/astrodados3/splus/idr6/HYDRA-0011/HYDRA-0011_F378.fits.fz\n",
    "    \"\"\"\n",
    "    for base in SPLUS_IMAGES_PATH:\n",
    "        path = os.path.join(base, field_name, f\"{field_name}_{band}.fits.fz\")\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    raise FileNotFoundError(\n",
    "        f\"No image found for field {field_name}, band {band} \"\n",
    "        f\"in any of {SPLUS_IMAGES_PATH}\"\n",
    "    )\n",
    "\n",
    "def process_chunk_splus_images(field_name,\n",
    "                               splus_chunk,\n",
    "                               cutout_size=SPLUS_CUTOUT_SIZE,\n",
    "                               mask_rows=None):\n",
    "    \"\"\"\n",
    "    For this S-PLUS chunk, generate cutouts in all SPLUS_BANDS and\n",
    "    return a DataFrame with columns of pixel data:\n",
    "\n",
    "        splus_cut_R, splus_cut_I, ..., splus_cut_F378, ...\n",
    "\n",
    "    Each cell is either:\n",
    "        - a 1D float32 array of length cutout_size*cutout_size,\n",
    "        - or None if cutout could not be made or was skipped.\n",
    "\n",
    "    If mask_rows is provided (Series aligned with splus_chunk.index),\n",
    "    cutouts are generated ONLY for rows where mask_rows is True.\n",
    "    \"\"\"\n",
    "    if splus_chunk.empty:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [SPLUS IMG] Making cutouts for chunk with {len(splus_chunk)} rows\")\n",
    "\n",
    "    # Normalize mask_rows to a boolean numpy array aligned to splus_chunk\n",
    "    mask_arr = None\n",
    "    if mask_rows is not None:\n",
    "        mask_rows = mask_rows.reindex(splus_chunk.index)\n",
    "        mask_arr = mask_rows.fillna(False).values\n",
    "\n",
    "    # Coordinates for all rows (we'll skip via mask if needed)\n",
    "    coords = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "\n",
    "    # Initialize result dict: one column per band\n",
    "    result = {f\"splus_cut_{b}\": [None] * len(splus_chunk) for b in SPLUS_BANDS}\n",
    "\n",
    "    # Iterate over bands, load image once per band\n",
    "    for band in SPLUS_BANDS:\n",
    "        col_name = f\"splus_cut_{band}\"\n",
    "        try:\n",
    "            img_path = get_splus_image_path_for_band(field_name, band)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"      [SPLUS IMG] {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"      [SPLUS IMG] Using image {img_path} for band {band}\")\n",
    "        try:\n",
    "            with fits.open(img_path, memmap=True) as hdul:\n",
    "                # Image is known to be in HDU=1\n",
    "                if len(hdul) <= 1 or hdul[1].data is None:\n",
    "                    print(f\"      [SPLUS IMG] HDU 1 has no data in {img_path}, skipping band {band}\")\n",
    "                    continue\n",
    "                image_hdu = hdul[1]\n",
    "                data = image_hdu.data\n",
    "                hdr = image_hdu.header\n",
    "                wcs = WCS(hdr)\n",
    "        except Exception as e:\n",
    "            print(f\"      [SPLUS IMG] Failed to open {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # For each source in chunk, try to make a cutout\n",
    "        for pos, coord in enumerate(coords):\n",
    "            # if mask is provided and this row has no Gaia match (or whatever criteria), skip\n",
    "            if mask_arr is not None and not mask_arr[pos]:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                cut = Cutout2D(data, position=coord, wcs=wcs, size=cutout_size)\n",
    "            except Exception:\n",
    "                # outside image or WCS error\n",
    "                continue\n",
    "\n",
    "            # Store flattened float32 array directly in the DataFrame\n",
    "            result[col_name][pos] = cut.data.astype(\"float32\").ravel()\n",
    "\n",
    "    cutout_df = pd.DataFrame(result, index=splus_chunk.index)\n",
    "    return cutout_df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Per-chunk external survey processing (Gaia / XP / ZTF / DESI / APOGEE)\n",
    "# -------------------------------------------------\n",
    "\n",
    "def process_chunk_gaia(splus_chunk, gaia_index):\n",
    "    if splus_chunk.empty:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [Gaia] Processing chunk with {len(splus_chunk)} S-PLUS rows\")\n",
    "\n",
    "    splus_coord = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    hpix_gaia = radec_to_hpix(\n",
    "        splus_chunk[\"ra\"], splus_chunk[\"dec\"], GAIA_NSIDE, GAIA_NEST\n",
    "    )\n",
    "\n",
    "    needed_files = files_for_hpix_ranges(hpix_gaia, gaia_index)\n",
    "    print(f\"      [Gaia] Files needed for this chunk: {len(needed_files)}\")\n",
    "    if not needed_files:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    gaia_rows = []\n",
    "    for path in needed_files:\n",
    "        df = read_gaia_ecsv(path)\n",
    "        gaia_rows.append(df)\n",
    "\n",
    "    gaia_cat = pd.concat(gaia_rows, ignore_index=True)\n",
    "    print(f\"      [Gaia] Total Gaia rows loaded for this chunk: {len(gaia_cat)}\")\n",
    "\n",
    "    if \"ra\" not in gaia_cat.columns or \"dec\" not in gaia_cat.columns:\n",
    "        raise ValueError(\"Gaia catalog has no 'ra'/'dec' columns.\")\n",
    "\n",
    "    gaia_coord = SkyCoord(\n",
    "        gaia_cat[\"ra\"].values * u.deg,\n",
    "        gaia_cat[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    match_idx = match_catalogs(splus_coord, gaia_coord, MATCH_RADIUS_SPLUS_GAIA)\n",
    "    n_matches = np.sum(match_idx != -1)\n",
    "    print(f\"      [Gaia] Matched {n_matches} / {len(splus_chunk)}\")\n",
    "\n",
    "    out = []\n",
    "    for idx in match_idx:\n",
    "        out.append({} if idx == -1 else gaia_cat.iloc[int(idx)].to_dict())\n",
    "\n",
    "    gaia_df = pd.DataFrame(out, index=splus_chunk.index)\n",
    "    if not gaia_df.empty:\n",
    "        gaia_df.columns = [f\"gaia_{c}\" for c in gaia_df.columns]\n",
    "\n",
    "    return gaia_df\n",
    "\n",
    "def process_chunk_gaia_spectra(gaia_df, gaia_spec_index):\n",
    "    if gaia_df.empty or \"gaia_source_id\" not in gaia_df.columns:\n",
    "        print(\"      [GaiaXP] No Gaia matches or missing 'gaia_source_id'.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    sid_series = gaia_df[\"gaia_source_id\"].dropna()\n",
    "    if sid_series.empty:\n",
    "        print(\"      [GaiaXP] No valid source_ids.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    if not {\"gaia_ra\", \"gaia_dec\"}.issubset(gaia_df.columns):\n",
    "        print(\"      [GaiaXP] Missing 'gaia_ra'/'gaia_dec'.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    ra = gaia_df[\"gaia_ra\"].astype(float)\n",
    "    dec = gaia_df[\"gaia_dec\"].astype(float)\n",
    "    mask_valid = (~ra.isna()) & (~dec.isna()) & (~gaia_df[\"gaia_source_id\"].isna())\n",
    "    if not mask_valid.any():\n",
    "        print(\"      [GaiaXP] No rows with valid RA/DEC + source_id.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    hpix_spec = radec_to_hpix(\n",
    "        ra[mask_valid].values,\n",
    "        dec[mask_valid].values,\n",
    "        GAIA_SPEC_NSIDE,\n",
    "        GAIA_SPEC_NEST,\n",
    "    )\n",
    "    needed_files = files_for_hpix_ranges(hpix_spec, gaia_spec_index)\n",
    "    print(f\"      [GaiaXP] Files needed: {len(needed_files)}\")\n",
    "\n",
    "    if not needed_files:\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    needed_source_ids = set(sid_series.astype(\"int64\").values.tolist())\n",
    "\n",
    "    spec_rows = []\n",
    "    for path in needed_files:\n",
    "        df = read_gaia_ecsv(path)\n",
    "        if \"source_id\" not in df.columns:\n",
    "            print(f\"        [GaiaXP] 'source_id' missing in {path}, skipping.\")\n",
    "            continue\n",
    "        df = df[df[\"source_id\"].isin(needed_source_ids)]\n",
    "        print(f\"        [GaiaXP] {len(df)} rows match in {path}\")\n",
    "        if not df.empty:\n",
    "            spec_rows.append(df)\n",
    "\n",
    "    if not spec_rows:\n",
    "        print(\"      [GaiaXP] No XP spectra matched.\")\n",
    "        return pd.DataFrame(index=gaia_df.index)\n",
    "\n",
    "    spec_cat = pd.concat(spec_rows, ignore_index=True)\n",
    "    spec_unique = spec_cat.drop_duplicates(subset=\"source_id\", keep=\"first\")\n",
    "    spec_unique = spec_unique.set_index(\"source_id\")\n",
    "\n",
    "    out = []\n",
    "    for sid in gaia_df[\"gaia_source_id\"]:\n",
    "        if pd.isna(sid):\n",
    "            out.append({})\n",
    "        else:\n",
    "            sid_int = int(sid)\n",
    "            if sid_int in spec_unique.index:\n",
    "                out.append(spec_unique.loc[sid_int].to_dict())\n",
    "            else:\n",
    "                out.append({})\n",
    "\n",
    "    gaiaxp_df = pd.DataFrame(out, index=gaia_df.index)\n",
    "    if not gaiaxp_df.empty:\n",
    "        gaiaxp_df.columns = [f\"gaiaxp_{c}\" for c in gaiaxp_df.columns]\n",
    "\n",
    "    return gaiaxp_df\n",
    "\n",
    "def process_chunk_ztf(splus_chunk):\n",
    "    if splus_chunk.empty:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [ZTF] Processing chunk with {len(splus_chunk)} S-PLUS rows\")\n",
    "\n",
    "    splus_coord = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    hpix_ztf = radec_to_hpix(\n",
    "        splus_chunk[\"ra\"], splus_chunk[\"dec\"], ZTF_NSIDE, ZTF_NEST\n",
    "    )\n",
    "\n",
    "    files_by_hpix = defaultdict(list)\n",
    "    for hpix in np.unique(hpix_ztf):\n",
    "        path = ztf_file_for_hpix(hpix)\n",
    "        if os.path.exists(path):\n",
    "            files_by_hpix[int(hpix)].append(path)\n",
    "\n",
    "    print(f\"      [ZTF] Unique hpix with existing files: {len(files_by_hpix)}\")\n",
    "\n",
    "    if not files_by_hpix:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    ztf_rows = []\n",
    "    for hpix, paths in files_by_hpix.items():\n",
    "        print(f\"        [ZTF] hpix={hpix} → {len(paths)} parquet files\")\n",
    "        for path in paths:\n",
    "            tab = pq.read_table(path).to_pandas()\n",
    "            ztf_rows.append(tab)\n",
    "\n",
    "    if not ztf_rows:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    ztf_cat = pd.concat(ztf_rows, ignore_index=True)\n",
    "    print(f\"      [ZTF] Total ZTF rows loaded: {len(ztf_cat)}\")\n",
    "\n",
    "    if \"ra\" not in ztf_cat.columns or \"dec\" not in ztf_cat.columns:\n",
    "        raise ValueError(\"ZTF parquet must contain 'ra'/'dec' columns.\")\n",
    "\n",
    "    ztf_coord = SkyCoord(\n",
    "        ztf_cat[\"ra\"].values * u.deg,\n",
    "        ztf_cat[\"dec\"].values * u.deg,\n",
    "    )\n",
    "    match_idx = match_catalogs(splus_coord, ztf_coord, MATCH_RADIUS_SPLUS_ZTF)\n",
    "    n_matches = np.sum(match_idx != -1)\n",
    "    print(f\"      [ZTF] Matched {n_matches} / {len(splus_chunk)}\")\n",
    "\n",
    "    out = []\n",
    "    for idx in match_idx:\n",
    "        if idx == -1:\n",
    "            out.append({})\n",
    "        else:\n",
    "            out.append(ztf_cat.iloc[int(idx)].to_dict())\n",
    "\n",
    "    ztf_df = pd.DataFrame(out, index=splus_chunk.index)\n",
    "    if not ztf_df.empty:\n",
    "        ztf_df.columns = [f\"ztf_{c}\" for c in ztf_df.columns]\n",
    "\n",
    "    return ztf_df\n",
    "\n",
    "def process_chunk_desi(splus_chunk, desi_tile_ids):\n",
    "    if splus_chunk.empty:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [DESI] Processing chunk with {len(splus_chunk)} S-PLUS rows\")\n",
    "    print(f\"      [DESI] Tiles considered for this field: {len(desi_tile_ids)}\")\n",
    "\n",
    "    if not desi_tile_ids:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    splus_coord = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "\n",
    "    desi_rows = [ {} for _ in range(len(splus_chunk)) ]\n",
    "\n",
    "    paths = desi_file_paths_for_tiles(desi_tile_ids)\n",
    "    print(f\"      [DESI] Total DESI coadd files to inspect: {len(paths)}\")\n",
    "\n",
    "    for path in paths:\n",
    "        print(f\"        [DESI] Opening {path}\")\n",
    "        with fits.open(path) as hd:\n",
    "            if \"FIBERMAP\" not in hd:\n",
    "                print(\"        [DESI] No FIBERMAP extension, skipping.\")\n",
    "                continue\n",
    "            fibermap = Table(hd[\"FIBERMAP\"].data).to_pandas()\n",
    "\n",
    "        # Drop fibermap rows with NaN coordinates\n",
    "        if \"TARGET_RA\" not in fibermap.columns or \"TARGET_DEC\" not in fibermap.columns:\n",
    "            print(\"        [DESI] FIBERMAP missing TARGET_RA/DEC, skipping.\")\n",
    "            continue\n",
    "\n",
    "        fibermap = fibermap.dropna(subset=[\"TARGET_RA\", \"TARGET_DEC\"])\n",
    "        if fibermap.empty:\n",
    "            print(\"        [DESI] All TARGET_RA/DEC are NaN in this file, skipping.\")\n",
    "            continue\n",
    "\n",
    "        desi_coord = SkyCoord(\n",
    "            fibermap[\"TARGET_RA\"].values * u.deg,\n",
    "            fibermap[\"TARGET_DEC\"].values * u.deg,\n",
    "        )\n",
    "        match_idx = match_catalogs(splus_coord, desi_coord, MATCH_RADIUS_SPLUS_DESI)\n",
    "        n_matches = np.sum(match_idx != -1)\n",
    "        print(f\"        [DESI] Matched {n_matches} / {len(splus_chunk)} in this file\")\n",
    "\n",
    "        for i, idx in enumerate(match_idx):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            if not desi_rows[i]:\n",
    "                row = dict(fibermap.iloc[int(idx)])\n",
    "                row[\"desi_file\"] = path\n",
    "                desi_rows[i] = row\n",
    "\n",
    "    desi_df = pd.DataFrame(desi_rows, index=splus_chunk.index)\n",
    "    if not desi_df.empty:\n",
    "        desi_df.columns = [f\"desi_{c}\" for c in desi_df.columns]\n",
    "\n",
    "    return desi_df\n",
    "\n",
    "def process_chunk_apogee(splus_chunk, apogee_cat, apogee_coord):\n",
    "    \"\"\"\n",
    "    Cross-match this S-PLUS chunk with APOGEE allStar DR17 (single big table).\n",
    "\n",
    "    apogee_cat:  DataFrame for all APOGEE rows (already loaded once in main)\n",
    "    apogee_coord: SkyCoord(APOGEE_RA, APOGEE_DEC)\n",
    "    \"\"\"\n",
    "    if splus_chunk.empty or apogee_cat is None or apogee_coord is None:\n",
    "        return pd.DataFrame(index=splus_chunk.index)\n",
    "\n",
    "    print(f\"      [APOGEE] Processing chunk with {len(splus_chunk)} S-PLUS rows\")\n",
    "\n",
    "    splus_coord = SkyCoord(\n",
    "        splus_chunk[\"ra\"].values * u.deg,\n",
    "        splus_chunk[\"dec\"].values * u.deg,\n",
    "    )\n",
    "\n",
    "    match_idx = match_catalogs(splus_coord, apogee_coord, MATCH_RADIUS_SPLUS_APOGEE)\n",
    "    n_matches = np.sum(match_idx != -1)\n",
    "    print(f\"      [APOGEE] Matched {n_matches} / {len(splus_chunk)}\")\n",
    "\n",
    "    out = []\n",
    "    for idx in match_idx:\n",
    "        out.append({} if idx == -1 else apogee_cat.iloc[int(idx)].to_dict())\n",
    "\n",
    "    apogee_df = pd.DataFrame(out, index=splus_chunk.index)\n",
    "    if not apogee_df.empty:\n",
    "        apogee_df.columns = [f\"apogee_{c}\" for c in apogee_df.columns]\n",
    "\n",
    "    return apogee_df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Per-field datacube\n",
    "# -------------------------------------------------\n",
    "\n",
    "def build_datacube_for_field(field_row,\n",
    "                             gaia_index,\n",
    "                             gaia_spec_index,\n",
    "                             desi_tiles,\n",
    "                             desi_tiles_coord,\n",
    "                             apogee_cat,\n",
    "                             apogee_coord):\n",
    "    field_name = field_row[\"field\"]\n",
    "    field_ra = float(field_row[\"ra\"])\n",
    "    field_dec = float(field_row[\"dec\"])\n",
    "\n",
    "    print(f\"\\n=== Field {field_name} ({field_ra}, {field_dec}) ===\")\n",
    "\n",
    "    # 0. Determine DESI tiles near this field (based on field center)\n",
    "    field_center = SkyCoord(field_ra * u.deg, field_dec * u.deg)\n",
    "    sep = field_center.separation(desi_tiles_coord)\n",
    "    mask_tiles = sep.deg < DESI_TILE_RADIUS_DEG\n",
    "    field_tile_ids = desi_tiles.loc[mask_tiles, \"TILEID\"].astype(int).tolist()\n",
    "    print(f\"  [DESI] Tiles within {DESI_TILE_RADIUS_DEG} deg of field center: {len(field_tile_ids)}\")\n",
    "\n",
    "    # 1. Open ONLY the S-PLUS catalog for this field\n",
    "    splus_field = load_splus_for_field(field_row)\n",
    "    print(f\"  [SPLUS] Objects in this field file: {len(splus_field)}\")\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    # 2. Process this field in chunks (for memory)\n",
    "    for start in range(0, len(splus_field), CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, len(splus_field))\n",
    "        print(f\"    [Field {field_name}] chunk {start}-{end}\")\n",
    "        chunk = splus_field.iloc[start:end].copy()\n",
    "\n",
    "        # Gaia first (so we can decide which rows get cutouts)\n",
    "        gaia_df   = process_chunk_gaia(chunk, gaia_index)\n",
    "\n",
    "        # Mask for cutouts: only rows with Gaia matches (if enabled)\n",
    "        cutout_mask = None\n",
    "        if ONLY_GAIA_MATCH_CUTOUTS and not gaia_df.empty and \"gaia_source_id\" in gaia_df.columns:\n",
    "            cutout_mask = ~gaia_df[\"gaia_source_id\"].isna()\n",
    "            print(f\"    [SPLUS IMG] Gaia-matched rows for cutouts: {cutout_mask.sum()} / {len(cutout_mask)}\")\n",
    "\n",
    "        # S-PLUS cutouts stored directly in parquet\n",
    "        splus_img_df = process_chunk_splus_images(field_name, chunk, mask_rows=cutout_mask)\n",
    "\n",
    "        # Gaia XP, ZTF, DESI, APOGEE\n",
    "        gaiaxp_df = process_chunk_gaia_spectra(gaia_df, gaia_spec_index)\n",
    "        ztf_df    = process_chunk_ztf(chunk)\n",
    "        desi_df   = process_chunk_desi(chunk, field_tile_ids)\n",
    "        apogee_df = process_chunk_apogee(chunk, apogee_cat, apogee_coord)\n",
    "\n",
    "        merged = (chunk\n",
    "                  .join(splus_img_df)\n",
    "                  .join(gaia_df)\n",
    "                  .join(gaiaxp_df)\n",
    "                  .join(ztf_df)\n",
    "                  .join(desi_df)\n",
    "                  .join(apogee_df))\n",
    "        all_chunks.append(merged)\n",
    "\n",
    "    datacube_field = pd.concat(all_chunks, ignore_index=True)\n",
    "    out_name = f\"datacube_{field_name}.parquet\"\n",
    "    datacube_field.to_parquet(out_name)\n",
    "    print(f\"  [OUT] Saved {len(datacube_field)} rows to {out_name}\")\n",
    "\n",
    "    # free memory for this field\n",
    "    del datacube_field\n",
    "    del all_chunks\n",
    "    del splus_field\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main\n",
    "# -------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    # Driver list\n",
    "    dr6_splus = pd.read_csv(DR6_LIST)\n",
    "    dr6_splus = dr6_splus[dr6_splus['field'] == 'STRIPE82-0001']\n",
    "    print(f\"Loaded {len(dr6_splus)} rows from {DR6_LIST}\")\n",
    "\n",
    "    # Gaia indices (HEALPix level-8 ranges, small metadata kept once)\n",
    "    gaia_index = build_range_index(GAIA_DIR, \"GaiaSource_\", \".csv.gz\")\n",
    "    print(f\"Indexed {len(gaia_index)} Gaia source chunks\")\n",
    "\n",
    "    gaia_spec_index = build_range_index(\n",
    "        GAIA_SPEC_DIR, \"XpContinuousMeanSpectrum_\", \".csv.gz\"\n",
    "    )\n",
    "    print(f\"Indexed {len(gaia_spec_index)} Gaia XP spectra chunks\")\n",
    "\n",
    "    # DESI tiles\n",
    "    desi_tiles = pd.read_csv(DESI_TILES_CSV)\n",
    "    # Expect TILEID, TILERA, TILEDEC\n",
    "    for col in (\"TILEID\", \"TILERA\", \"TILEDEC\"):\n",
    "        if col not in desi_tiles.columns:\n",
    "            raise ValueError(f\"{DESI_TILES_CSV} is missing '{col}' column.\")\n",
    "    print(f\"Loaded {len(desi_tiles)} DESI tiles from {DESI_TILES_CSV}\")\n",
    "\n",
    "    desi_tiles_coord = SkyCoord(\n",
    "        desi_tiles[\"TILERA\"].values * u.deg,\n",
    "        desi_tiles[\"TILEDEC\"].values * u.deg,\n",
    "    )\n",
    "\n",
    "    # APOGEE allStar DR17 (drop multidimensional columns before to_pandas)\n",
    "    apogee_cat = None\n",
    "    apogee_coord = None\n",
    "    if os.path.exists(APOGEE_PATH):\n",
    "        print(f\"Loading APOGEE catalog from {APOGEE_PATH}\")\n",
    "        tab = Table.read(APOGEE_PATH)\n",
    "\n",
    "        # Keep only 1D columns (drop FPARAM, FELEM, etc.)\n",
    "        names_1d = [name for name in tab.colnames if tab[name].ndim <= 1]\n",
    "        tab_1d = tab[names_1d]\n",
    "\n",
    "        apogee_cat = tab_1d.to_pandas()\n",
    "\n",
    "        # Ensure RA/DEC exist and drop NaNs\n",
    "        if \"RA\" not in apogee_cat.columns or \"DEC\" not in apogee_cat.columns:\n",
    "            raise ValueError(\"APOGEE catalog has no 'RA'/'DEC' columns.\")\n",
    "\n",
    "        mask = (~apogee_cat[\"RA\"].isna()) & (~apogee_cat[\"DEC\"].isna())\n",
    "        apogee_cat = apogee_cat.loc[mask].reset_index(drop=True)\n",
    "        apogee_coord = SkyCoord(\n",
    "            apogee_cat[\"RA\"].values * u.deg,\n",
    "            apogee_cat[\"DEC\"].values * u.deg,\n",
    "        )\n",
    "        print(f\"Loaded {len(apogee_cat)} APOGEE rows with valid RA/DEC and 1D columns\")\n",
    "    else:\n",
    "        print(f\"[APOGEE] File not found: {APOGEE_PATH} (APOGEE will be skipped)\")\n",
    "\n",
    "    # Process ONE FIELD AT A TIME\n",
    "    for _, row in dr6_splus.iterrows():\n",
    "        build_datacube_for_field(row,\n",
    "                                 gaia_index,\n",
    "                                 gaia_spec_index,\n",
    "                                 desi_tiles,\n",
    "                                 desi_tiles_coord,\n",
    "                                 apogee_cat,\n",
    "                                 apogee_coord)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb371171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d883f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"datacube_HYDRA-0011.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'random_idx',\n",
       " 'ra',\n",
       " 'dec',\n",
       " 'field',\n",
       " 'a_pixel_det',\n",
       " 'a_restricted_pixel_r',\n",
       " 'b_pixel_det',\n",
       " 'b_restricted_pixel_r',\n",
       " 'class_star_det',\n",
       " 'class_star_g',\n",
       " 'class_star_i',\n",
       " 'class_star_j0378',\n",
       " 'class_star_j0395',\n",
       " 'class_star_j0410',\n",
       " 'class_star_j0430',\n",
       " 'class_star_j0515',\n",
       " 'class_star_j0660',\n",
       " 'class_star_j0861',\n",
       " 'class_star_r',\n",
       " 'class_star_u',\n",
       " 'class_star_z',\n",
       " 'ellipticity_det',\n",
       " 'elongation_det',\n",
       " 'err_a_pixel_det',\n",
       " 'err_b_pixel_det',\n",
       " 'err_mag_aper_3_g',\n",
       " 'err_mag_aper_3_i',\n",
       " 'err_mag_aper_3_j0378',\n",
       " 'err_mag_aper_3_j0395',\n",
       " 'err_mag_aper_3_j0410',\n",
       " 'err_mag_aper_3_j0430',\n",
       " 'err_mag_aper_3_j0515',\n",
       " 'err_mag_aper_3_j0660',\n",
       " 'err_mag_aper_3_j0861',\n",
       " 'err_mag_aper_3_r',\n",
       " 'err_mag_aper_3_u',\n",
       " 'err_mag_aper_3_z',\n",
       " 'err_mag_aper_6_g',\n",
       " 'err_mag_aper_6_i',\n",
       " 'err_mag_aper_6_j0378',\n",
       " 'err_mag_aper_6_j0395',\n",
       " 'err_mag_aper_6_j0410',\n",
       " 'err_mag_aper_6_j0430',\n",
       " 'err_mag_aper_6_j0515',\n",
       " 'err_mag_aper_6_j0660',\n",
       " 'err_mag_aper_6_j0861',\n",
       " 'err_mag_aper_6_r',\n",
       " 'err_mag_aper_6_u',\n",
       " 'err_mag_aper_6_z',\n",
       " 'err_mag_auto_g',\n",
       " 'err_mag_auto_i',\n",
       " 'err_mag_auto_j0378',\n",
       " 'err_mag_auto_j0395',\n",
       " 'err_mag_auto_j0410',\n",
       " 'err_mag_auto_j0430',\n",
       " 'err_mag_auto_j0515',\n",
       " 'err_mag_auto_j0660',\n",
       " 'err_mag_auto_j0861',\n",
       " 'err_mag_auto_r',\n",
       " 'err_mag_auto_u',\n",
       " 'err_mag_auto_z',\n",
       " 'err_mag_isophotal_g',\n",
       " 'err_mag_isophotal_i',\n",
       " 'err_mag_isophotal_j0378',\n",
       " 'err_mag_isophotal_j0395',\n",
       " 'err_mag_isophotal_j0410',\n",
       " 'err_mag_isophotal_j0430',\n",
       " 'err_mag_isophotal_j0515',\n",
       " 'err_mag_isophotal_j0660',\n",
       " 'err_mag_isophotal_j0861',\n",
       " 'err_mag_isophotal_r',\n",
       " 'err_mag_isophotal_u',\n",
       " 'err_mag_isophotal_z',\n",
       " 'err_mag_petro_g',\n",
       " 'err_mag_petro_i',\n",
       " 'err_mag_petro_j0378',\n",
       " 'err_mag_petro_j0395',\n",
       " 'err_mag_petro_j0410',\n",
       " 'err_mag_petro_j0430',\n",
       " 'err_mag_petro_j0515',\n",
       " 'err_mag_petro_j0660',\n",
       " 'err_mag_petro_j0861',\n",
       " 'err_mag_petro_r',\n",
       " 'err_mag_petro_u',\n",
       " 'err_mag_petro_z',\n",
       " 'err_mag_psf_g',\n",
       " 'err_mag_psf_i',\n",
       " 'err_mag_psf_j0378',\n",
       " 'err_mag_psf_j0395',\n",
       " 'err_mag_psf_j0410',\n",
       " 'err_mag_psf_j0430',\n",
       " 'err_mag_psf_j0515',\n",
       " 'err_mag_psf_j0660',\n",
       " 'err_mag_psf_j0861',\n",
       " 'err_mag_psf_r',\n",
       " 'err_mag_psf_u',\n",
       " 'err_mag_psf_z',\n",
       " 'err_mag_pstotal_g',\n",
       " 'err_mag_pstotal_i',\n",
       " 'err_mag_pstotal_j0378',\n",
       " 'err_mag_pstotal_j0395',\n",
       " 'err_mag_pstotal_j0410',\n",
       " 'err_mag_pstotal_j0430',\n",
       " 'err_mag_pstotal_j0515',\n",
       " 'err_mag_pstotal_j0660',\n",
       " 'err_mag_pstotal_j0861',\n",
       " 'err_mag_pstotal_r',\n",
       " 'err_mag_pstotal_u',\n",
       " 'err_mag_pstotal_z',\n",
       " 'flags_det',\n",
       " 'flags_g',\n",
       " 'flags_i',\n",
       " 'flags_j0378',\n",
       " 'flags_j0395',\n",
       " 'flags_j0410',\n",
       " 'flags_j0430',\n",
       " 'flags_j0515',\n",
       " 'flags_j0660',\n",
       " 'flags_j0861',\n",
       " 'flags_r',\n",
       " 'flags_u',\n",
       " 'flags_z',\n",
       " 'flux_radius_as_20_det',\n",
       " 'flux_radius_as_20_g',\n",
       " 'flux_radius_as_20_i',\n",
       " 'flux_radius_as_20_j0378',\n",
       " 'flux_radius_as_20_j0395',\n",
       " 'flux_radius_as_20_j0410',\n",
       " 'flux_radius_as_20_j0430',\n",
       " 'flux_radius_as_20_j0515',\n",
       " 'flux_radius_as_20_j0660',\n",
       " 'flux_radius_as_20_j0861',\n",
       " 'flux_radius_as_20_r',\n",
       " 'flux_radius_as_20_u',\n",
       " 'flux_radius_as_20_z',\n",
       " 'flux_radius_as_50_det',\n",
       " 'flux_radius_as_50_g',\n",
       " 'flux_radius_as_50_i',\n",
       " 'flux_radius_as_50_j0378',\n",
       " 'flux_radius_as_50_j0395',\n",
       " 'flux_radius_as_50_j0410',\n",
       " 'flux_radius_as_50_j0430',\n",
       " 'flux_radius_as_50_j0515',\n",
       " 'flux_radius_as_50_j0660',\n",
       " 'flux_radius_as_50_j0861',\n",
       " 'flux_radius_as_50_r',\n",
       " 'flux_radius_as_50_u',\n",
       " 'flux_radius_as_50_z',\n",
       " 'flux_radius_as_70_det',\n",
       " 'flux_radius_as_70_g',\n",
       " 'flux_radius_as_70_i',\n",
       " 'flux_radius_as_70_j0378',\n",
       " 'flux_radius_as_70_j0395',\n",
       " 'flux_radius_as_70_j0410',\n",
       " 'flux_radius_as_70_j0430',\n",
       " 'flux_radius_as_70_j0515',\n",
       " 'flux_radius_as_70_j0660',\n",
       " 'flux_radius_as_70_j0861',\n",
       " 'flux_radius_as_70_r',\n",
       " 'flux_radius_as_70_u',\n",
       " 'flux_radius_as_70_z',\n",
       " 'flux_radius_as_90_det',\n",
       " 'flux_radius_as_90_g',\n",
       " 'flux_radius_as_90_i',\n",
       " 'flux_radius_as_90_j0378',\n",
       " 'flux_radius_as_90_j0395',\n",
       " 'flux_radius_as_90_j0410',\n",
       " 'flux_radius_as_90_j0430',\n",
       " 'flux_radius_as_90_j0515',\n",
       " 'flux_radius_as_90_j0660',\n",
       " 'flux_radius_as_90_j0861',\n",
       " 'flux_radius_as_90_r',\n",
       " 'flux_radius_as_90_u',\n",
       " 'flux_radius_as_90_z',\n",
       " 'fwhm_n_det',\n",
       " 'fwhm_n_g',\n",
       " 'fwhm_n_i',\n",
       " 'fwhm_n_j0378',\n",
       " 'fwhm_n_j0395',\n",
       " 'fwhm_n_j0410',\n",
       " 'fwhm_n_j0430',\n",
       " 'fwhm_n_j0515',\n",
       " 'fwhm_n_j0660',\n",
       " 'fwhm_n_j0861',\n",
       " 'fwhm_n_r',\n",
       " 'fwhm_n_u',\n",
       " 'fwhm_n_z',\n",
       " 'fwhm_pixels_det',\n",
       " 'fwhm_pixels_g',\n",
       " 'fwhm_pixels_i',\n",
       " 'fwhm_pixels_j0378',\n",
       " 'fwhm_pixels_j0395',\n",
       " 'fwhm_pixels_j0410',\n",
       " 'fwhm_pixels_j0430',\n",
       " 'fwhm_pixels_j0515',\n",
       " 'fwhm_pixels_j0660',\n",
       " 'fwhm_pixels_j0861',\n",
       " 'fwhm_pixels_r',\n",
       " 'fwhm_pixels_u',\n",
       " 'fwhm_pixels_z',\n",
       " 'isophotal_area_pixel_det',\n",
       " 'isophotal_area_pixel_g',\n",
       " 'isophotal_area_pixel_i',\n",
       " 'isophotal_area_pixel_j0378',\n",
       " 'isophotal_area_pixel_j0395',\n",
       " 'isophotal_area_pixel_j0410',\n",
       " 'isophotal_area_pixel_j0430',\n",
       " 'isophotal_area_pixel_j0515',\n",
       " 'isophotal_area_pixel_j0660',\n",
       " 'isophotal_area_pixel_j0861',\n",
       " 'isophotal_area_pixel_r',\n",
       " 'isophotal_area_pixel_u',\n",
       " 'isophotal_area_pixel_z',\n",
       " 'kron_radius_det',\n",
       " 'kron_radius_restricted_r',\n",
       " 'mag_aper_3_g',\n",
       " 'mag_aper_3_i',\n",
       " 'mag_aper_3_j0378',\n",
       " 'mag_aper_3_j0395',\n",
       " 'mag_aper_3_j0410',\n",
       " 'mag_aper_3_j0430',\n",
       " 'mag_aper_3_j0515',\n",
       " 'mag_aper_3_j0660',\n",
       " 'mag_aper_3_j0861',\n",
       " 'mag_aper_3_r',\n",
       " 'mag_aper_3_u',\n",
       " 'mag_aper_3_z',\n",
       " 'mag_aper_6_g',\n",
       " 'mag_aper_6_i',\n",
       " 'mag_aper_6_j0378',\n",
       " 'mag_aper_6_j0395',\n",
       " 'mag_aper_6_j0410',\n",
       " 'mag_aper_6_j0430',\n",
       " 'mag_aper_6_j0515',\n",
       " 'mag_aper_6_j0660',\n",
       " 'mag_aper_6_j0861',\n",
       " 'mag_aper_6_r',\n",
       " 'mag_aper_6_u',\n",
       " 'mag_aper_6_z',\n",
       " 'mag_auto_g',\n",
       " 'mag_auto_i',\n",
       " 'mag_auto_j0378',\n",
       " 'mag_auto_j0395',\n",
       " 'mag_auto_j0410',\n",
       " 'mag_auto_j0430',\n",
       " 'mag_auto_j0515',\n",
       " 'mag_auto_j0660',\n",
       " 'mag_auto_j0861',\n",
       " 'mag_auto_r',\n",
       " 'mag_auto_restricted_g',\n",
       " 'mag_auto_restricted_i',\n",
       " 'mag_auto_restricted_j0378',\n",
       " 'mag_auto_restricted_j0395',\n",
       " 'mag_auto_restricted_j0410',\n",
       " 'mag_auto_restricted_j0430',\n",
       " 'mag_auto_restricted_j0515',\n",
       " 'mag_auto_restricted_j0660',\n",
       " 'mag_auto_restricted_j0861',\n",
       " 'mag_auto_restricted_r',\n",
       " 'mag_auto_restricted_u',\n",
       " 'mag_auto_restricted_z',\n",
       " 'mag_auto_u',\n",
       " 'mag_auto_z',\n",
       " 'mag_isophotal_g',\n",
       " 'mag_isophotal_i',\n",
       " 'mag_isophotal_j0378',\n",
       " 'mag_isophotal_j0395',\n",
       " 'mag_isophotal_j0410',\n",
       " 'mag_isophotal_j0430',\n",
       " 'mag_isophotal_j0515',\n",
       " 'mag_isophotal_j0660',\n",
       " 'mag_isophotal_j0861',\n",
       " 'mag_isophotal_r',\n",
       " 'mag_isophotal_u',\n",
       " 'mag_isophotal_z',\n",
       " 'mag_petro_g',\n",
       " 'mag_petro_i',\n",
       " 'mag_petro_j0378',\n",
       " 'mag_petro_j0395',\n",
       " 'mag_petro_j0410',\n",
       " 'mag_petro_j0430',\n",
       " 'mag_petro_j0515',\n",
       " 'mag_petro_j0660',\n",
       " 'mag_petro_j0861',\n",
       " 'mag_petro_r',\n",
       " 'mag_petro_u',\n",
       " 'mag_petro_z',\n",
       " 'mag_psf_g',\n",
       " 'mag_psf_i',\n",
       " 'mag_psf_j0378',\n",
       " 'mag_psf_j0395',\n",
       " 'mag_psf_j0410',\n",
       " 'mag_psf_j0430',\n",
       " 'mag_psf_j0515',\n",
       " 'mag_psf_j0660',\n",
       " 'mag_psf_j0861',\n",
       " 'mag_psf_r',\n",
       " 'mag_psf_u',\n",
       " 'mag_psf_z',\n",
       " 'mag_pstotal_g',\n",
       " 'mag_pstotal_i',\n",
       " 'mag_pstotal_j0378',\n",
       " 'mag_pstotal_j0395',\n",
       " 'mag_pstotal_j0410',\n",
       " 'mag_pstotal_j0430',\n",
       " 'mag_pstotal_j0515',\n",
       " 'mag_pstotal_j0660',\n",
       " 'mag_pstotal_j0861',\n",
       " 'mag_pstotal_r',\n",
       " 'mag_pstotal_u',\n",
       " 'mag_pstotal_z',\n",
       " 'mu_background_g',\n",
       " 'mu_background_i',\n",
       " 'mu_background_j0378',\n",
       " 'mu_background_j0395',\n",
       " 'mu_background_j0410',\n",
       " 'mu_background_j0430',\n",
       " 'mu_background_j0515',\n",
       " 'mu_background_j0660',\n",
       " 'mu_background_j0861',\n",
       " 'mu_background_r',\n",
       " 'mu_background_u',\n",
       " 'mu_background_z',\n",
       " 'mu_max_g',\n",
       " 'mu_max_i',\n",
       " 'mu_max_j0378',\n",
       " 'mu_max_j0395',\n",
       " 'mu_max_j0410',\n",
       " 'mu_max_j0430',\n",
       " 'mu_max_j0515',\n",
       " 'mu_max_j0660',\n",
       " 'mu_max_j0861',\n",
       " 'mu_max_r',\n",
       " 'mu_max_u',\n",
       " 'mu_max_z',\n",
       " 'mu_threshold_g',\n",
       " 'mu_threshold_i',\n",
       " 'mu_threshold_j0378',\n",
       " 'mu_threshold_j0395',\n",
       " 'mu_threshold_j0410',\n",
       " 'mu_threshold_j0430',\n",
       " 'mu_threshold_j0515',\n",
       " 'mu_threshold_j0660',\n",
       " 'mu_threshold_j0861',\n",
       " 'mu_threshold_r',\n",
       " 'mu_threshold_u',\n",
       " 'mu_threshold_z',\n",
       " 'petro_radius_det',\n",
       " 's2n_aper_3_det',\n",
       " 's2n_aper_3_g',\n",
       " 's2n_aper_3_i',\n",
       " 's2n_aper_3_j0378',\n",
       " 's2n_aper_3_j0395',\n",
       " 's2n_aper_3_j0410',\n",
       " 's2n_aper_3_j0430',\n",
       " 's2n_aper_3_j0515',\n",
       " 's2n_aper_3_j0660',\n",
       " 's2n_aper_3_j0861',\n",
       " 's2n_aper_3_r',\n",
       " 's2n_aper_3_u',\n",
       " 's2n_aper_3_z',\n",
       " 's2n_aper_6_det',\n",
       " 's2n_aper_6_g',\n",
       " 's2n_aper_6_i',\n",
       " 's2n_aper_6_j0378',\n",
       " 's2n_aper_6_j0395',\n",
       " 's2n_aper_6_j0410',\n",
       " 's2n_aper_6_j0430',\n",
       " 's2n_aper_6_j0515',\n",
       " 's2n_aper_6_j0660',\n",
       " 's2n_aper_6_j0861',\n",
       " 's2n_aper_6_r',\n",
       " 's2n_aper_6_u',\n",
       " 's2n_aper_6_z',\n",
       " 's2n_auto_det',\n",
       " 's2n_auto_g',\n",
       " 's2n_auto_i',\n",
       " 's2n_auto_j0378',\n",
       " 's2n_auto_j0395',\n",
       " 's2n_auto_j0410',\n",
       " 's2n_auto_j0430',\n",
       " 's2n_auto_j0515',\n",
       " 's2n_auto_j0660',\n",
       " 's2n_auto_j0861',\n",
       " 's2n_auto_r',\n",
       " 's2n_auto_u',\n",
       " 's2n_auto_z',\n",
       " 's2n_iso_det',\n",
       " 's2n_iso_g',\n",
       " 's2n_iso_i',\n",
       " 's2n_iso_j0378',\n",
       " 's2n_iso_j0395',\n",
       " 's2n_iso_j0410',\n",
       " 's2n_iso_j0430',\n",
       " 's2n_iso_j0515',\n",
       " 's2n_iso_j0660',\n",
       " 's2n_iso_j0861',\n",
       " 's2n_iso_r',\n",
       " 's2n_iso_u',\n",
       " 's2n_iso_z',\n",
       " 's2n_petro_det',\n",
       " 's2n_petro_g',\n",
       " 's2n_petro_i',\n",
       " 's2n_petro_j0378',\n",
       " 's2n_petro_j0395',\n",
       " 's2n_petro_j0410',\n",
       " 's2n_petro_j0430',\n",
       " 's2n_petro_j0515',\n",
       " 's2n_petro_j0660',\n",
       " 's2n_petro_j0861',\n",
       " 's2n_petro_r',\n",
       " 's2n_petro_u',\n",
       " 's2n_petro_z',\n",
       " 's2n_psf_g',\n",
       " 's2n_psf_i',\n",
       " 's2n_psf_j0378',\n",
       " 's2n_psf_j0395',\n",
       " 's2n_psf_j0410',\n",
       " 's2n_psf_j0430',\n",
       " 's2n_psf_j0515',\n",
       " 's2n_psf_j0660',\n",
       " 's2n_psf_j0861',\n",
       " 's2n_psf_r',\n",
       " 's2n_psf_u',\n",
       " 's2n_psf_z',\n",
       " 's2n_pstotal_det',\n",
       " 's2n_pstotal_g',\n",
       " 's2n_pstotal_i',\n",
       " 's2n_pstotal_j0378',\n",
       " 's2n_pstotal_j0395',\n",
       " 's2n_pstotal_j0410',\n",
       " 's2n_pstotal_j0430',\n",
       " 's2n_pstotal_j0515',\n",
       " 's2n_pstotal_j0660',\n",
       " 's2n_pstotal_j0861',\n",
       " 's2n_pstotal_r',\n",
       " 's2n_pstotal_u',\n",
       " 's2n_pstotal_z',\n",
       " 'theta_det',\n",
       " 'x_pixel_det',\n",
       " 'y_pixel_det',\n",
       " 'splus_cut_R',\n",
       " 'splus_cut_I',\n",
       " 'splus_cut_G',\n",
       " 'splus_cut_U',\n",
       " 'splus_cut_Z',\n",
       " 'splus_cut_F378',\n",
       " 'splus_cut_F395',\n",
       " 'splus_cut_F410',\n",
       " 'splus_cut_F430',\n",
       " 'splus_cut_F515',\n",
       " 'splus_cut_F660',\n",
       " 'splus_cut_F861',\n",
       " 'gaia_solution_id',\n",
       " 'gaia_designation',\n",
       " 'gaia_source_id',\n",
       " 'gaia_random_index',\n",
       " 'gaia_ref_epoch',\n",
       " 'gaia_ra',\n",
       " 'gaia_ra_error',\n",
       " 'gaia_dec',\n",
       " 'gaia_dec_error',\n",
       " 'gaia_parallax',\n",
       " 'gaia_parallax_error',\n",
       " 'gaia_parallax_over_error',\n",
       " 'gaia_pm',\n",
       " 'gaia_pmra',\n",
       " 'gaia_pmra_error',\n",
       " 'gaia_pmdec',\n",
       " 'gaia_pmdec_error',\n",
       " 'gaia_ra_dec_corr',\n",
       " 'gaia_ra_parallax_corr',\n",
       " 'gaia_ra_pmra_corr',\n",
       " 'gaia_ra_pmdec_corr',\n",
       " 'gaia_dec_parallax_corr',\n",
       " 'gaia_dec_pmra_corr',\n",
       " 'gaia_dec_pmdec_corr',\n",
       " 'gaia_parallax_pmra_corr',\n",
       " 'gaia_parallax_pmdec_corr',\n",
       " 'gaia_pmra_pmdec_corr',\n",
       " 'gaia_astrometric_n_obs_al',\n",
       " 'gaia_astrometric_n_obs_ac',\n",
       " 'gaia_astrometric_n_good_obs_al',\n",
       " 'gaia_astrometric_n_bad_obs_al',\n",
       " 'gaia_astrometric_gof_al',\n",
       " 'gaia_astrometric_chi2_al',\n",
       " 'gaia_astrometric_excess_noise',\n",
       " 'gaia_astrometric_excess_noise_sig',\n",
       " 'gaia_astrometric_params_solved',\n",
       " 'gaia_astrometric_primary_flag',\n",
       " 'gaia_nu_eff_used_in_astrometry',\n",
       " 'gaia_pseudocolour',\n",
       " 'gaia_pseudocolour_error',\n",
       " 'gaia_ra_pseudocolour_corr',\n",
       " 'gaia_dec_pseudocolour_corr',\n",
       " 'gaia_parallax_pseudocolour_corr',\n",
       " 'gaia_pmra_pseudocolour_corr',\n",
       " 'gaia_pmdec_pseudocolour_corr',\n",
       " 'gaia_astrometric_matched_transits',\n",
       " 'gaia_visibility_periods_used',\n",
       " 'gaia_astrometric_sigma5d_max',\n",
       " 'gaia_matched_transits',\n",
       " 'gaia_new_matched_transits',\n",
       " 'gaia_matched_transits_removed',\n",
       " 'gaia_ipd_gof_harmonic_amplitude',\n",
       " 'gaia_ipd_gof_harmonic_phase',\n",
       " 'gaia_ipd_frac_multi_peak',\n",
       " 'gaia_ipd_frac_odd_win',\n",
       " 'gaia_ruwe',\n",
       " 'gaia_scan_direction_strength_k1',\n",
       " 'gaia_scan_direction_strength_k2',\n",
       " 'gaia_scan_direction_strength_k3',\n",
       " 'gaia_scan_direction_strength_k4',\n",
       " 'gaia_scan_direction_mean_k1',\n",
       " 'gaia_scan_direction_mean_k2',\n",
       " 'gaia_scan_direction_mean_k3',\n",
       " 'gaia_scan_direction_mean_k4',\n",
       " 'gaia_duplicated_source',\n",
       " 'gaia_phot_g_n_obs',\n",
       " 'gaia_phot_g_mean_flux',\n",
       " 'gaia_phot_g_mean_flux_error',\n",
       " 'gaia_phot_g_mean_flux_over_error',\n",
       " 'gaia_phot_g_mean_mag',\n",
       " 'gaia_phot_bp_n_obs',\n",
       " 'gaia_phot_bp_mean_flux',\n",
       " 'gaia_phot_bp_mean_flux_error',\n",
       " 'gaia_phot_bp_mean_flux_over_error',\n",
       " 'gaia_phot_bp_mean_mag',\n",
       " 'gaia_phot_rp_n_obs',\n",
       " 'gaia_phot_rp_mean_flux',\n",
       " 'gaia_phot_rp_mean_flux_error',\n",
       " 'gaia_phot_rp_mean_flux_over_error',\n",
       " 'gaia_phot_rp_mean_mag',\n",
       " 'gaia_phot_bp_rp_excess_factor',\n",
       " 'gaia_phot_bp_n_contaminated_transits',\n",
       " 'gaia_phot_bp_n_blended_transits',\n",
       " 'gaia_phot_rp_n_contaminated_transits',\n",
       " 'gaia_phot_rp_n_blended_transits',\n",
       " 'gaia_phot_proc_mode',\n",
       " 'gaia_bp_rp',\n",
       " 'gaia_bp_g',\n",
       " 'gaia_g_rp',\n",
       " 'gaia_radial_velocity',\n",
       " 'gaia_radial_velocity_error',\n",
       " 'gaia_rv_method_used',\n",
       " 'gaia_rv_nb_transits',\n",
       " 'gaia_rv_nb_deblended_transits',\n",
       " 'gaia_rv_visibility_periods_used',\n",
       " 'gaia_rv_expected_sig_to_noise',\n",
       " 'gaia_rv_renormalised_gof',\n",
       " 'gaia_rv_chisq_pvalue',\n",
       " 'gaia_rv_time_duration',\n",
       " 'gaia_rv_amplitude_robust',\n",
       " 'gaia_rv_template_teff',\n",
       " 'gaia_rv_template_logg',\n",
       " 'gaia_rv_template_fe_h',\n",
       " 'gaia_rv_atm_param_origin',\n",
       " 'gaia_vbroad',\n",
       " 'gaia_vbroad_error',\n",
       " 'gaia_vbroad_nb_transits',\n",
       " 'gaia_grvs_mag',\n",
       " 'gaia_grvs_mag_error',\n",
       " 'gaia_grvs_mag_nb_transits',\n",
       " 'gaia_rvs_spec_sig_to_noise',\n",
       " 'gaia_phot_variable_flag',\n",
       " 'gaia_l',\n",
       " 'gaia_b',\n",
       " 'gaia_ecl_lon',\n",
       " 'gaia_ecl_lat',\n",
       " 'gaia_in_qso_candidates',\n",
       " 'gaia_in_galaxy_candidates',\n",
       " 'gaia_non_single_star',\n",
       " 'gaia_has_xp_continuous',\n",
       " 'gaia_has_xp_sampled',\n",
       " 'gaia_has_rvs',\n",
       " 'gaia_has_epoch_photometry',\n",
       " 'gaia_has_epoch_rv',\n",
       " 'gaia_has_mcmc_gspphot',\n",
       " 'gaia_has_mcmc_msc',\n",
       " 'gaia_in_andromeda_survey',\n",
       " 'gaia_classprob_dsc_combmod_quasar',\n",
       " 'gaia_classprob_dsc_combmod_galaxy',\n",
       " 'gaia_classprob_dsc_combmod_star',\n",
       " 'gaia_teff_gspphot',\n",
       " 'gaia_teff_gspphot_lower',\n",
       " 'gaia_teff_gspphot_upper',\n",
       " 'gaia_logg_gspphot',\n",
       " 'gaia_logg_gspphot_lower',\n",
       " 'gaia_logg_gspphot_upper',\n",
       " 'gaia_mh_gspphot',\n",
       " 'gaia_mh_gspphot_lower',\n",
       " 'gaia_mh_gspphot_upper',\n",
       " 'gaia_distance_gspphot',\n",
       " 'gaia_distance_gspphot_lower',\n",
       " 'gaia_distance_gspphot_upper',\n",
       " 'gaia_azero_gspphot',\n",
       " 'gaia_azero_gspphot_lower',\n",
       " 'gaia_azero_gspphot_upper',\n",
       " 'gaia_ag_gspphot',\n",
       " 'gaia_ag_gspphot_lower',\n",
       " 'gaia_ag_gspphot_upper',\n",
       " 'gaia_ebpminrp_gspphot',\n",
       " 'gaia_ebpminrp_gspphot_lower',\n",
       " 'gaia_ebpminrp_gspphot_upper',\n",
       " 'gaia_libname_gspphot',\n",
       " 'gaiaxp_solution_id',\n",
       " 'gaiaxp_bp_basis_function_id',\n",
       " 'gaiaxp_bp_degrees_of_freedom',\n",
       " 'gaiaxp_bp_n_parameters',\n",
       " 'gaiaxp_bp_n_measurements',\n",
       " 'gaiaxp_bp_n_rejected_measurements',\n",
       " 'gaiaxp_bp_standard_deviation',\n",
       " 'gaiaxp_bp_chi_squared',\n",
       " 'gaiaxp_bp_coefficients',\n",
       " 'gaiaxp_bp_coefficient_errors',\n",
       " 'gaiaxp_bp_coefficient_correlations',\n",
       " 'gaiaxp_bp_n_relevant_bases',\n",
       " 'gaiaxp_bp_relative_shrinking',\n",
       " 'gaiaxp_rp_basis_function_id',\n",
       " 'gaiaxp_rp_degrees_of_freedom',\n",
       " 'gaiaxp_rp_n_parameters',\n",
       " 'gaiaxp_rp_n_measurements',\n",
       " 'gaiaxp_rp_n_rejected_measurements',\n",
       " 'gaiaxp_rp_standard_deviation',\n",
       " 'gaiaxp_rp_chi_squared',\n",
       " 'gaiaxp_rp_coefficients',\n",
       " 'gaiaxp_rp_coefficient_errors',\n",
       " 'gaiaxp_rp_coefficient_correlations',\n",
       " 'gaiaxp_rp_n_relevant_bases',\n",
       " 'gaiaxp_rp_relative_shrinking']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f48db09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              NaN\n",
       "1              NaN\n",
       "2              NaN\n",
       "3        22.006819\n",
       "4              NaN\n",
       "           ...    \n",
       "77122          NaN\n",
       "77123    19.555763\n",
       "77124          NaN\n",
       "77125          NaN\n",
       "77126          NaN\n",
       "Name: gaia_phot_bp_mean_mag, Length: 77127, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gaia_phot_bp_mean_mag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1501700e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     None\n",
       "1                                                     None\n",
       "2                                                     None\n",
       "3        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4                                                     None\n",
       "                               ...                        \n",
       "77122                                                 None\n",
       "77123    [0.07828938, 0.034619495, -0.11812654, -0.2108...\n",
       "77124                                                 None\n",
       "77125                                                 None\n",
       "77126                                                 None\n",
       "Name: splus_cut_R, Length: 77127, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"splus_cut_R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c9d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
