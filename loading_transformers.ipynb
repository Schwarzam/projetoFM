{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa45c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6acf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dr6_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0075f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = df['field'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bae168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "image_codecs = \"codecs/image_tokens/datacube_{field}_tokens.npz\"\n",
    "scalar_codecs = \"scalar_tokenizers/scalar_tokens/datacube_{field}_scalar_tokens.npz\"\n",
    "spectrum_tokens = \"spectrum_tokenizers/spectrum_tokens/datacube_{field}_spectrum_tokens.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561085d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacube = \"/home/astrodados4/downloads/hypercube/datacube_{field}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "434deb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------\n",
    "# ID normalizers (same idea)\n",
    "# ---------------------------\n",
    "\n",
    "def norm_splus_id(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        return x.decode(\"utf-8\", errors=\"ignore\")\n",
    "    return str(x)\n",
    "\n",
    "def norm_gaia_id(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return None\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Tokenizer label LUT\n",
    "# ---------------------------\n",
    "\n",
    "def load_scalar_tokenizer_config(config_path: str):\n",
    "    \"\"\"\n",
    "    Loads your scalar_tokenizer_config.npz and returns:\n",
    "      - bin_edges: dict[col] -> edges (N_BINS+1,)\n",
    "      - n_bins: int\n",
    "    \"\"\"\n",
    "    data = np.load(config_path, allow_pickle=True)\n",
    "    bin_edges_raw = data[\"bin_edges\"].item()\n",
    "    n_bins = int(data[\"N_BINS\"])\n",
    "\n",
    "    # keys might be bytes -> normalize\n",
    "    bin_edges = {}\n",
    "    for k, v in bin_edges_raw.items():\n",
    "        kk = norm_splus_id(k)\n",
    "        bin_edges[kk] = np.asarray(v, dtype=np.float64)\n",
    "\n",
    "    return bin_edges, n_bins\n",
    "\n",
    "def build_edges_lut_for_scalar_cols(bin_edges: dict, scalar_cols: np.ndarray, n_bins: int):\n",
    "    \"\"\"\n",
    "    Returns a list `edges_list` aligned with scalar_cols:\n",
    "      edges_list[j] is float64 edges array for scalar_cols[j], or None if missing.\n",
    "    \"\"\"\n",
    "    edges_list = []\n",
    "    for c in scalar_cols:\n",
    "        col = norm_splus_id(c)\n",
    "        e = bin_edges.get(col, None)\n",
    "        if e is None or len(e) != (n_bins + 1):\n",
    "            edges_list.append(None)\n",
    "        else:\n",
    "            edges_list.append(e)\n",
    "    return edges_list\n",
    "\n",
    "def decode_scalar_row_to_labels(tokens_row: np.ndarray, scalar_cols: np.ndarray, edges_list: list):\n",
    "    \"\"\"\n",
    "    tokens_row: (n_cols,) uint16\n",
    "    returns list of dicts: [{\"col\":..., \"tok\":..., \"lo\":..., \"hi\":...}, ...]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for j, tok in enumerate(tokens_row):\n",
    "        e = edges_list[j]\n",
    "        col = norm_splus_id(scalar_cols[j])\n",
    "        t = int(tok)\n",
    "\n",
    "        if e is None:\n",
    "            out.append({\"col\": col, \"tok\": t, \"lo\": None, \"hi\": None})\n",
    "            continue\n",
    "\n",
    "        # clamp just in case\n",
    "        if t < 0:\n",
    "            t = 0\n",
    "        if t > len(e) - 2:\n",
    "            t = len(e) - 2\n",
    "\n",
    "        lo = float(e[t])\n",
    "        hi = float(e[t + 1])\n",
    "        out.append({\"col\": col, \"tok\": t, \"lo\": lo, \"hi\": hi})\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# NPZ helpers\n",
    "# ---------------------------\n",
    "\n",
    "def npz_row_map_by_id(npz, token_key: str, id_normalizer):\n",
    "    \"\"\"\n",
    "    dict[id] -> token_row\n",
    "    \"\"\"\n",
    "    if \"ids\" not in npz.files or token_key not in npz.files:\n",
    "        return {}\n",
    "    ids = [id_normalizer(x) for x in np.asarray(npz[\"ids\"], dtype=object)]\n",
    "    toks = npz[token_key]\n",
    "    return {ids[i]: toks[i] for i in range(len(ids))}\n",
    "\n",
    "def build_ragged_map_from_npz(npz, ids_key: str, flat_key: str, indptr_key: str, id_normalizer):\n",
    "    \"\"\"\n",
    "    dict[id] -> 1D token sequence (ragged) using flat+indptr CSR-style.\n",
    "    \"\"\"\n",
    "    if ids_key not in npz.files or flat_key not in npz.files or indptr_key not in npz.files:\n",
    "        return {}\n",
    "    ids = [id_normalizer(x) for x in np.asarray(npz[ids_key], dtype=object)]\n",
    "    flat = npz[flat_key]\n",
    "    indptr = npz[indptr_key]\n",
    "    out = {}\n",
    "    for i, key in enumerate(ids):\n",
    "        a = int(indptr[i])\n",
    "        b = int(indptr[i + 1])\n",
    "        out[key] = flat[a:b]\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN STORE BUILDER\n",
    "# ---------------------------\n",
    "\n",
    "def build_object_store_for_field(\n",
    "    field: str,\n",
    "    datacube_tmpl: str,\n",
    "    image_npz_tmpl: str,\n",
    "    scalar_npz_tmpl: str,\n",
    "    spectrum_npz_tmpl: str,\n",
    "    scalar_tokenizer_config_path: str,   # <<--- NEW\n",
    "    image_token_key: str = \"tokens_flat\",\n",
    "    decode_scalar_labels: bool = True,   # <<--- NEW\n",
    "):\n",
    "    # 1) read metadata and filter\n",
    "    df = pd.read_parquet(\n",
    "        datacube_tmpl.format(field=field),\n",
    "        columns=[\"id\", \"ra\", \"dec\", \"gaia_source_id\", \"mag_pstotal_r\", \"err_mag_pstotal_r\"],\n",
    "    )\n",
    "\n",
    "    mask = (\n",
    "        (df[\"mag_pstotal_r\"] < 22.0)\n",
    "        & (df[\"mag_pstotal_r\"] > 14.0)\n",
    "        & (df[\"err_mag_pstotal_r\"] < 2.0)\n",
    "    )\n",
    "    df = df.loc[mask].copy()\n",
    "    df[\"id\"] = df[\"id\"].map(norm_splus_id)\n",
    "    df[\"gaia_source_id\"] = df[\"gaia_source_id\"].map(norm_gaia_id)\n",
    "\n",
    "    print(f\"[{field}] selected sources: {len(df)}\")\n",
    "\n",
    "    # 2) load npz files\n",
    "    img = np.load(image_npz_tmpl.format(field=field), allow_pickle=True)\n",
    "    sca = np.load(scalar_npz_tmpl.format(field=field), allow_pickle=True)\n",
    "    spe = np.load(spectrum_npz_tmpl.format(field=field), allow_pickle=True)\n",
    "\n",
    "    print(f\"[{field}] image npz keys   = {list(img.files)}\")\n",
    "    print(f\"[{field}] scalar npz keys  = {list(sca.files)}\")\n",
    "    print(f\"[{field}] spectrum npz keys= {list(spe.files)}\")\n",
    "\n",
    "    # 3) maps\n",
    "    # image keyed by Gaia id\n",
    "    img_map = npz_row_map_by_id(img, token_key=image_token_key, id_normalizer=norm_gaia_id)\n",
    "\n",
    "    # scalar keyed by S-PLUS id, tokens are (N, n_scalar_cols)\n",
    "    if \"ids\" not in sca.files or \"scalar_tokens\" not in sca.files or \"scalar_cols\" not in sca.files:\n",
    "        raise RuntimeError(\"Scalar NPZ must contain ids, scalar_tokens, scalar_cols.\")\n",
    "\n",
    "    sca_ids = [norm_splus_id(x) for x in np.asarray(sca[\"ids\"], dtype=object)]\n",
    "    sca_tokens = sca[\"scalar_tokens\"]\n",
    "    sca_cols = np.asarray(sca[\"scalar_cols\"], dtype=object)\n",
    "\n",
    "    sca_map = {sca_ids[i]: sca_tokens[i] for i in range(len(sca_ids))}\n",
    "\n",
    "    # spectrum keyed by S-PLUS id (ragged)\n",
    "    bp_map = build_ragged_map_from_npz(\n",
    "        spe,\n",
    "        ids_key=\"ids\",\n",
    "        flat_key=\"tokens_gaiaxp_bp_flat\",\n",
    "        indptr_key=\"tokens_gaiaxp_bp_indptr\",\n",
    "        id_normalizer=norm_splus_id,\n",
    "    )\n",
    "    rp_map = build_ragged_map_from_npz(\n",
    "        spe,\n",
    "        ids_key=\"ids\",\n",
    "        flat_key=\"tokens_gaiaxp_rp_flat\",\n",
    "        indptr_key=\"tokens_gaiaxp_rp_indptr\",\n",
    "        id_normalizer=norm_splus_id,\n",
    "    )\n",
    "\n",
    "    # 4) tokenizer LUT (for decoding scalar tokens -> (lo,hi))\n",
    "    bin_edges, n_bins = load_scalar_tokenizer_config(scalar_tokenizer_config_path)\n",
    "    edges_list = build_edges_lut_for_scalar_cols(bin_edges, sca_cols, n_bins)\n",
    "\n",
    "    # 5) merge store (keyed by S-PLUS id)\n",
    "    store = {}\n",
    "    for r in df.itertuples(index=False):\n",
    "        sid = r.id\n",
    "        gid = r.gaia_source_id\n",
    "\n",
    "        scalar_row = sca_map.get(sid, None)\n",
    "\n",
    "        store[sid] = {\n",
    "            \"meta\": {\n",
    "                \"field\": field,\n",
    "                \"id\": sid,  # S-PLUS id (string)\n",
    "                \"ra\": float(r.ra),\n",
    "                \"dec\": float(r.dec),\n",
    "                \"gaia_source_id\": gid,  # int or None\n",
    "                \"mag_pstotal_r\": float(r.mag_pstotal_r),\n",
    "                \"err_mag_pstotal_r\": float(r.err_mag_pstotal_r),\n",
    "            },\n",
    "            \"image_tokens\": (img_map.get(gid, None) if gid is not None else None),\n",
    "            \"scalar_tokens\": scalar_row,\n",
    "            \"scalar_cols\": sca_cols,  # keep col order for later use\n",
    "            \"scalar_labels\": (decode_scalar_row_to_labels(scalar_row, sca_cols, edges_list)\n",
    "                              if (decode_scalar_labels and scalar_row is not None) else None),\n",
    "            \"spectrum_tokens\": {\n",
    "                \"gaiaxp_bp\": bp_map.get(sid, None),\n",
    "                \"gaiaxp_rp\": rp_map.get(sid, None),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # coverage report\n",
    "    n = len(store)\n",
    "    n_img = sum(v[\"image_tokens\"] is not None for v in store.values())\n",
    "    n_sca = sum(v[\"scalar_tokens\"] is not None for v in store.values())\n",
    "    n_bp = sum(v[\"spectrum_tokens\"][\"gaiaxp_bp\"] is not None and len(v[\"spectrum_tokens\"][\"gaiaxp_bp\"]) > 0 for v in store.values())\n",
    "    n_rp = sum(v[\"spectrum_tokens\"][\"gaiaxp_rp\"] is not None and len(v[\"spectrum_tokens\"][\"gaiaxp_rp\"]) > 0 for v in store.values())\n",
    "\n",
    "    print(f\"[{field}] coverage: image={n_img}/{n} scalar={n_sca}/{n} bp={n_bp}/{n} rp={n_rp}/{n}\")\n",
    "\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4dae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field: HYDRA-0011, Selected sources: 36491\n",
      "Field: HYDRA-0011\n",
      "Image tokens shape: (19331,)\n",
      "Scalar tokens shape: (36496,)\n",
      "Spectrum tokens shape: (77127,)\n",
      "[HYDRA-0011] selected sources: 36491\n",
      "[HYDRA-0011] image npz keys   = ['tokens_2d', 'tokens_flat', 'ids', 'id_col']\n",
      "[HYDRA-0011] scalar npz keys  = ['scalar_tokens', 'scalar_cols', 'ids', 'id_col', 'N_BINS']\n",
      "[HYDRA-0011] spectrum npz keys= ['ids', 'id_col', 'tokens_gaiaxp_bp_flat', 'tokens_gaiaxp_bp_indptr', 'tokens_gaiaxp_rp_flat', 'tokens_gaiaxp_rp_indptr']\n",
      "[HYDRA-0011] coverage: image=18768/36491 scalar=36491/36491 bp=608/36491 rp=608/36491\n",
      "Field: HYDRA-0012, Selected sources: 34632\n",
      "Field: HYDRA-0012\n",
      "Image tokens shape: (18431,)\n",
      "Scalar tokens shape: (34656,)\n",
      "Spectrum tokens shape: (80758,)\n",
      "[HYDRA-0012] selected sources: 34632\n",
      "[HYDRA-0012] image npz keys   = ['tokens_2d', 'tokens_flat', 'ids', 'id_col']\n",
      "[HYDRA-0012] scalar npz keys  = ['scalar_tokens', 'scalar_cols', 'ids', 'id_col', 'N_BINS']\n",
      "[HYDRA-0012] spectrum npz keys= ['ids', 'id_col', 'tokens_gaiaxp_bp_flat', 'tokens_gaiaxp_bp_indptr', 'tokens_gaiaxp_rp_flat', 'tokens_gaiaxp_rp_indptr']\n",
      "[HYDRA-0012] coverage: image=17969/34632 scalar=34632/34632 bp=484/34632 rp=484/34632\n",
      "Field: HYDRA-0013, Selected sources: 37162\n",
      "Field: HYDRA-0013\n",
      "Image tokens shape: (18334,)\n",
      "Scalar tokens shape: (37186,)\n",
      "Spectrum tokens shape: (73104,)\n",
      "[HYDRA-0013] selected sources: 37162\n",
      "[HYDRA-0013] image npz keys   = ['tokens_2d', 'tokens_flat', 'ids', 'id_col']\n",
      "[HYDRA-0013] scalar npz keys  = ['scalar_tokens', 'scalar_cols', 'ids', 'id_col', 'N_BINS']\n",
      "[HYDRA-0013] spectrum npz keys= ['ids', 'id_col', 'tokens_gaiaxp_bp_flat', 'tokens_gaiaxp_bp_indptr', 'tokens_gaiaxp_rp_flat', 'tokens_gaiaxp_rp_indptr']\n",
      "[HYDRA-0013] coverage: image=17834/37162 scalar=37162/37162 bp=519/37162 rp=519/37162\n",
      "Field: HYDRA-0014, Selected sources: 34967\n",
      "Field: HYDRA-0014\n",
      "Image tokens shape: (17364,)\n",
      "Scalar tokens shape: (35028,)\n",
      "Spectrum tokens shape: (80204,)\n",
      "[HYDRA-0014] selected sources: 34967\n",
      "[HYDRA-0014] image npz keys   = ['tokens_2d', 'tokens_flat', 'ids', 'id_col']\n",
      "[HYDRA-0014] scalar npz keys  = ['scalar_tokens', 'scalar_cols', 'ids', 'id_col', 'N_BINS']\n",
      "[HYDRA-0014] spectrum npz keys= ['ids', 'id_col', 'tokens_gaiaxp_bp_flat', 'tokens_gaiaxp_bp_indptr', 'tokens_gaiaxp_rp_flat', 'tokens_gaiaxp_rp_indptr']\n"
     ]
    }
   ],
   "source": [
    "for field in fields:\n",
    "    field_df = pd.read_parquet(datacube.format(field=field), columns=['id', 'ra', 'dec', 'gaia_source_id', 'mag_pstotal_r', 'err_mag_pstotal_r'])\n",
    "    mask = (field_df['mag_pstotal_r'] < 22.0) &\\\n",
    "    (field_df['mag_pstotal_r'] > 14.0) &\\\n",
    "    (field_df['err_mag_pstotal_r'] < 2)\n",
    "    \n",
    "    field_df = field_df[mask]\n",
    "    print(f\"Field: {field}, Selected sources: {len(field_df)}\")\n",
    "    \n",
    "    image_token = np.load(image_codecs.format(field=field), allow_pickle=True)\n",
    "    scalar_token = np.load(scalar_codecs.format(field=field), allow_pickle=True)\n",
    "    spectrum_token = np.load(spectrum_tokens.format(field=field), allow_pickle=True)\n",
    "    \n",
    "    print(f\"Field: {field}\")\n",
    "    print(f\"Image tokens shape: {image_token['ids'].shape}\")\n",
    "    print(f\"Scalar tokens shape: {scalar_token['ids'].shape}\")\n",
    "    print(f\"Spectrum tokens shape: {spectrum_token['ids'].shape}\")\n",
    "    \n",
    "    store = build_object_store_for_field(\n",
    "        field=field,\n",
    "        datacube_tmpl=datacube.format(field=field),\n",
    "        image_npz_tmpl=image_codecs.format(field=field),\n",
    "        scalar_npz_tmpl=scalar_codecs.format(field=field),\n",
    "        spectrum_npz_tmpl=spectrum_tokens.format(field=field),\n",
    "        scalar_tokenizer_config_path=\"scalar_tokenizers/scalar_tokenizer_config.npz\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d0f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
